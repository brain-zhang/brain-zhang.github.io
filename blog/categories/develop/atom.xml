<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Develop | Living a Simple Life is a Happy Life]]></title>
  <link href="https://happy123.me/blog/categories/develop/atom.xml" rel="self"/>
  <link href="https://happy123.me/"/>
  <updated>2020-10-21T09:45:31+08:00</updated>
  <id>https://happy123.me/</id>
  <author>
    <name><![CDATA[brain-zhang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[吴恩达机器学习笔记-5]]></title>
    <link href="https://happy123.me/blog/2019/09/22/wu-en-da-ji-qi-xue-xi-bi-ji-5/"/>
    <updated>2019-09-22T18:01:34+08:00</updated>
    <id>https://happy123.me/blog/2019/09/22/wu-en-da-ji-qi-xue-xi-bi-ji-5</id>
    <content type="html"><![CDATA[<p class="info">降维，异常检测，推荐系统，大规模机器学习</p>

<!-- more -->

<h2 id="section">数据压缩</h2>

<h4 id="section-1">降维问题</h4>

<p>假设我们未知两个的特征： 𝑥1 :长度, 用厘米表示； 𝑥2：是用英寸表示同一物体的长度。</p>

<p>这给了我们高度冗余表示，也许不是两个分开的特征  𝑥1  和  𝑥2 ，这两个基本的长度度量，我们可以减少数据到一维。</p>

<p>假使我们有有关于许多不同国家的数据，每一个特征向量都有 50 个特征（如，GDP，人均GDP，平均寿命等）。如果要将这个 50 维的数据可视化是不可能的。使用降维的方法将其降至 2 维，我们便可以将其可视化了。</p>

<h4 id="pca-">PCA 降维算法</h4>

<p>在 PCA 中，我们要做的是找到一个方向向量（Vector direction），
当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。
方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p>

<p>主成分分析与线性回归是两种不同的算法。
主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。
线性回归的目的是预测结果，而主成分分析不作任何预测。</p>

<p>过程：</p>

<ol>
  <li>均值归一化 (mean normalization)。计算出所有特征的均值，然后令 𝑥𝑗=𝑥𝑗−𝜇𝑗 。如果特征是在不同的数量级上，我们还需要将其除以标准差  𝜎2 。</li>
  <li>计算协方差矩阵（covariance matrix）Σ：</li>
  <li>是计算协方差矩阵 Σ 的特征向量（eigenvectors）: 可以利用奇异值分解（singular value decomposition 理解 SVD）来求解，[U, S, V]= svd(Σ)。</li>
</ol>

<p>```
import numpy as np</p>

<p>def covariance_matrix(X):
    “””
    Args:
        X (ndarray) (m, n)
    Return:
        cov_mat (ndarray) (n, n):
            covariance matrix of X
    “””
    m = X.shape[0]</p>

<pre><code>return (X.T @ X) / m
</code></pre>

<p>def normalize(X):
    “””
        for each column, X-mean / std
    “””
    X_copy = X.copy()
    m, n = X_copy.shape</p>

<pre><code>for col in range(n):
    X_copy[:, col] = (X_copy[:, col] - X_copy[:, col].mean()) / X_copy[:, col].std()

return X_copy
</code></pre>

<p>def pca(x, keep_dims=None):
    if not keep_dims:
        keep_dims = x.shape[1] - 1
    # 进行归一化
    normalize_x = normalize(x)
    # 求出协方差矩阵
    cov_x = covariance_matrix(x)
    # 奇异值分解
    U, S, V = np.linalg.svd(cov_x)  # U: principle components (n, n)
    # 选取前 keep_dims 维特征
    reduction = U[:, :keep_dims]
    # 得到降维的结果
    return np.matmul(x, reduction)</p>

<p>x = np.random.uniform(size=(10, 10))
pca(x).shape
```</p>

<h2 id="section-2">异常检测</h2>

<p>用途:</p>

<ol>
  <li>
    <p>识别欺骗。特征：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。构建模型来识别那些不符合该模式的用户,</p>
  </li>
  <li>
    <p>数据中心。特征：内存使用情况，被访问的磁盘数量，CPU 的负载，网络的通信量等。构建模型来判断某些计算机是不是有可能出错了。</p>
  </li>
</ol>

<h4 id="section-3">高斯分布</h4>

<p>如果变量 𝑥 符合高斯分布 𝑥 𝑁(𝜇,𝜎2) , 则其概率密度函数为：</p>

<script type="math/tex; mode=display">
p(x,\mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})
</script>

<h4 id="section-4">高斯分布的异常检测算法</h4>

<p>对于每一个样本值，计算特征，并以此估算高斯分布中的𝜇 和𝜎2的估计值;</p>

<p>以此来绘制一个估计函数，在这个估计函数之外的值即异常值；</p>

<p>模型计算  𝑝(𝑥) :</p>

<script type="math/tex; mode=display">
p(x)=\prod_{j=1}^n p(x_j;\mu_j,\sigma^2_j)=\prod_{j=1}^n \frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma^2_j})
</script>

<script type="math/tex; mode=display">
p(x)=\prod_{j=1}^n p(x_j;\mu_j,\sigma^2_j)=\prod_{j=1}^n \frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma^2_j})
</script>

<p>当  𝑝(𝑥)&lt;𝜀 时，为异常。</p>

<h4 id="section-5">开发和评价一个异常检测系统</h4>

<ol>
  <li>根据测试集数据，我们估计特征的平均值和方差并构建 𝑝(𝑥) 函数</li>
  <li>对交叉检验集，我们尝试使用不同的 𝜀 值作为阀值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择 𝜀</li>
  <li>选出 𝜀 后，针对测试集进行预测，计算异常检验系统的 F1 值，或者查准率与查全率之比。</li>
</ol>

<h4 id="section-6">特征选择</h4>

<p>我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用 CPU 负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。</p>

<h4 id="section-7">多元高斯分布</h4>

<p>TODO…</p>

<h2 id="section-8">推荐系统</h2>

<h4 id="section-9">协同过滤</h4>

<p>TODO….</p>

<h2 id="section-10">大规模机器学习</h2>

<h4 id="section-11">大型数据集的学习</h4>

<p>我们应该怎样应对一个有 100 万条记录的训练集？</p>

<p>以线性回归模型为例，每一次梯度下降迭代，我们都需要计算训练集的误差的平方和，如果我们的学习算法需要有 20 次迭代，这便已经是非常大的计算代价。</p>

<p>首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用 1000 个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。</p>

<h4 id="section-12">随机梯度下降</h4>

<p>随机梯度下降算法在每一次计算之后便更新参数 θ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。</p>

<h4 id="section-13">小批量梯度下降</h4>
<p>小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数 b 次训练实例，便更新一次参数 θ。</p>

<p>通常我们会令 b 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 b 个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。</p>

<h4 id="map-reduce--">Map Reduce 和 数据并行</h4>

<p>Map Reduce和数据并行对于大规模机器学习问题而言是非常重要的概念。</p>

<p>之前提到，如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果我们能够将我们的数据集分配给多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计算的结果汇总然后再求和。这样的方法叫做Map Reduce。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[吴恩达机器学习笔记-4]]></title>
    <link href="https://happy123.me/blog/2019/09/18/wu-en-da-ji-qi-xue-xi-bi-ji-4/"/>
    <updated>2019-09-18T16:59:17+08:00</updated>
    <id>https://happy123.me/blog/2019/09/18/wu-en-da-ji-qi-xue-xi-bi-ji-4</id>
    <content type="html"><![CDATA[<p class="info">机器学习系统设计思路，向量机，聚类</p>

<!-- more -->

<h2 id="section">机器学习系统设计</h2>

<h4 id="section-1">确定优先级</h4>

<ul>
  <li>如何设计一个垃圾邮件分类器算法?</li>
</ul>

<ol>
  <li>
    <p>首先，决定如何选择并表达特征向量x：可以选择一个由 100 个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为 1，不出现为 0），尺寸为 100×1。</p>
  </li>
  <li>收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本</li>
  <li>基于邮件的路由信息开发一系列复杂的特征</li>
  <li>基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理</li>
  <li>为探测刻意的拼写错误（例如: 把 watch 写成 w4tch）开发复杂的算法</li>
</ol>

<h4 id="section-2">误差分析</h4>

<p>构建一个学习算法的推荐方法为：</p>

<ol>
  <li>从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法</li>
  <li>绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择</li>
  <li>进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势</li>
</ol>

<h4 id="section-3">不对称分类的误差</h4>

<p>偏斜类（skewed classes）问题，表现为训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例。</p>

<ul>
  <li>
    <p>查准率（Precision） = TP/（TP+FP）。
例：肿瘤预测中，在所有预测有恶性肿瘤的病人中，实际上有恶性肿 瘤的病人的百分比，越高越好。</p>
  </li>
  <li>
    <p>查全率（Recall） = TP/（TP+FN）。
例：肿瘤预测中，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p>
  </li>
</ul>

<p>对于肿瘤预测来说, 查全率更重要</p>

<h4 id="section-4">精确率和召回率的权衡</h4>

<p>如果希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比 0.5 更大的阀值，如 0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。</p>

<p>如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比 0.5 更小的阀值，如 0.3。</p>

<p>选择阈值的一种方法是是计算 F1 值（F1 Score），其计算公式为：</p>

<script type="math/tex; mode=display">
F_1Score = 2\frac{PR}{P+R}
</script>

<h4 id="section-5">机器学习数据</h4>

<p>关于机器学习数据与特征值的选取比较有效的检测方法：</p>

<ol>
  <li>
    <p>一个人类专家看到了特征值 x，能很有信心的预测出 y 值吗？因为这可以证明 y 可以根据特征值 x 被准确地预测出来。</p>
  </li>
  <li>
    <p>我们实际上能得到一组庞大的训练集，并且在这个训练集中训练一个有很多参数的学习算法吗？</p>
  </li>
</ol>

<h2 id="section-6">向量机</h2>

<h4 id="section-7">支持向量机</h4>

<p>简称 SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。</p>

<p>老实说，向量机没有理解；它是作为一种分类器来使用的，他画出来的分类线比线性回归和逻辑回归的偏差更小；简称大间距分类器，意思是分类线的到每一个样本点的距离，都保持最大间隔，这样就跟具有鲁棒性，分的就明显；</p>

<h4 id="section-8">核函数</h4>

<p>TODO，待理解</p>

<h2 id="section-9">非监督学习</h2>

<h4 id="k-means">K-Means算法</h4>

<p>K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。假设我们想要将数据聚类成 n 个组，其方法为:</p>

<ol>
  <li>选择 k 个随机的点，称为聚类中心（cluster centroids）；</li>
  <li>对于数据集中的每一个数据，按照距离 K个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类；</li>
  <li>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置；</li>
  <li>重复步骤 2-4 直至中心点不再变化。</li>
</ol>

<h4 id="section-10">优化</h4>

<p>𝜇𝑐(𝑖) 代表与𝑥(𝑖) 最近的聚类中心点。优化目标便是找出使得代价函数最小的𝑐(1),𝑐(2),…𝑐(𝑚)和 𝜇1,𝜇2,…,𝜇𝑘。</p>

<ul>
  <li>K-均值迭代算法</li>
</ul>

<ol>
  <li>第一个循环(cluster assignment)是用于减小 𝑐(𝑖) 引起的代价</li>
  <li>第二个循环(move centroid)则是用于减小 𝜇𝑖 引起的代价。</li>
</ol>

<h4 id="section-11">随机初始化</h4>

<p>随机初始化所有的聚类中心点的做法：</p>

<ol>
  <li>我们应该选择 K &lt; m，即聚类中心点的个数要小于所有训练集实例的数量</li>
  <li>随机选择 K 个训练实例，然后令 K 个聚类中心分别与这 K 个训练实例相等</li>
</ol>

<h4 id="section-12">选择聚类数目</h4>

<p>改变 聚类数k 值，运行K-均值聚类方法，然后计算成本 函数或者计算畸变函数 J。</p>

<p>我们可能会得到一条这样像肘部的曲线，这就是“肘部法则”所做的。
这种模式下，它的畸变值会迅速下降，从 1 到 2，从 2 到 3 之后，你会在 3 的时候达到一个肘点。
在此之后，畸变值就下降的非常慢，我们就选这个转折点。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[吴恩达机器学习笔记-3]]></title>
    <link href="https://happy123.me/blog/2019/09/16/wu-en-da-ji-qi-xue-xi-bi-ji-3/"/>
    <updated>2019-09-16T16:55:56+08:00</updated>
    <id>https://happy123.me/blog/2019/09/16/wu-en-da-ji-qi-xue-xi-bi-ji-3</id>
    <content type="html"><![CDATA[<p class="info">神经网络学习, 反向传播算法， 模型优化</p>

<!-- more -->

<h2 id="section">神经网络学习</h2>

<h4 id="section-1">为什么需要神经网络</h4>

<p>普通的逻辑回归模型，不能有效地处理大量的特征，这时候我们需要神经网络。</p>

<h4 id="section-2">神经元和大脑</h4>

<p>大脑是个通用处理机，同样的一部分大脑区域，可以处理声音、视觉、味觉等多种信号；</p>

<p>从某种意义上来说，如果我们能找出大脑的学习算法，然后在计算机上执 行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。人工智能的梦想就是：有一天能制造出真正的智能机器。</p>

<h4 id="section-3">模型表示</h4>

<p><img src="https://raw.githubusercontent.com/brain-zhang/memoryboxes.github.io/source/images/20190916/bg1.jpg" alt="" /></p>

<p>第一层称为输入层（Input Layer），最后一 层称为输出层（Output Layer），中间一层称为隐藏层（Hidden Layers）。在神经网络中，参数又可被称为权重（weight）。我们为每一层都增加一个偏差单位（bias unit）</p>

<p>```
import numpy as np</p>

<p>def sigmoid(x):
    return 1/(1+np.exp(-x))</p>

<p>def net():
    # todo 确定输入和权重的维度
    X = np.array([[1],[-2],[3],[-4]])
    theta1 = np.random.uniform(size=(3, 4))
    hidden_input = sigmoid(np.matmul(theta1,X))
    print(‘hidden_input’,hidden_input)
    hidden_input = np.insert(hidden_input, 0, [1], axis=0)
    print(‘hidden_input’,hidden_input)
    theta2 = np.random.uniform(size=(1, 4))
    output = sigmoid(np.matmul( theta2,hidden_input))
    return output
```</p>

<h4 id="section-4">模型的表示实例</h4>

<p>从本质上讲，神经网络能够通过学习得出其自身的一系列特征。</p>

<p>在普通的逻辑回归中，我们被限制为使用数据中的原始特征 𝑥0,𝑥1,𝑥2,𝑥3 我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。</p>

<p>在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。</p>

<p>神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与（AND）、逻辑或（OR）。</p>

<p>```
# 实现AND函数
import numpy as np</p>

<p>def sigmoid(x):
    return 1/(1+np.exp(-x))</p>

<p>class Net():
    def <strong>init</strong>(self,theta):
        self.theta=theta
    def run(self,X): <br />
        output = sigmoid(np.matmul(self.theta,X))
        return output</p>

<p>net = Net(np.array([[-30,20,20]]))
```</p>

<h4 id="section-5">多分类任务</h4>

<p>one-hot的基本思想：将离散型特征的每一种取值都看成一种状态，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。</p>

<p>如果分类问题有四个结果，
我们不会将y的取值为：0，1，2，3 而是会将y表示为一个1*4的向量</p>

<p><code>
import numpy as np
import tqdm
def onehot(x):
    unique_values = list(set(x))
    number_of_dimension = len(unique_values)
    onehot_features = np.zeros(shape=(len(x), number_of_dimension))
    for row in tqdm.tqdm(range(len(x))):
        onehot_features[row, unique_values.index(x[row])] = 1
    return onehot_features
</code></p>

<h2 id="section-6">反向传播算法</h2>

<h4 id="section-7">代价函数</h4>

<p><img src="https://raw.githubusercontent.com/brain-zhang/memoryboxes.github.io/source/images/20190916/bg2.jpg" alt="" /></p>

<p>通过代价函数来观察算法预测的结果与真实情况的误差有多大，与逻辑回归唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环
在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。 </p>

<h4 id="section-8">反向传播</h4>

<p>计算神经网络预测结果的时候我们采用了一种正向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的 ℎ𝜃(𝑥) </p>

<p>计算代价函数的偏导数，我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。</p>

<p>老实说，反向传播的原理推导一直没搞明白，留一个通俗版先不求甚解：</p>

<p>https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/</p>

<p>最后，反向传播是为了提升神经网络学习模型中梯度下降的训练速度；是一种快速计算导数的方法；</p>

<h4 id="section-9">梯度校验</h4>

<p>名词跟梯度下降很相似，但是作用不一样；</p>

<p>当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。</p>

<p>为了避免这样的问题，我们采取一种叫做梯度的数值检验（Numerical Gradient Checking）的方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。</p>

<p>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 𝜃 ，我们计算出在  𝜃−𝜎 处和 𝜃+𝜎 的代价值（ 𝜎 是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 𝜃 处的代价值。</p>

<p>当 𝜃 是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对 𝜃1 进行检验的示例：</p>

<script type="math/tex; mode=display">
\frac{\partial}{\partial\theta_1} = \frac{J(\theta_1+\sigma_1,\theta_2,\theta_3,...,\theta_n)-J(\theta_1-\sigma_1,\theta_2,\theta_3,...,\theta_n)}{2\sigma}
</script>

<h4 id="section-10">随机初始化</h4>

<p>任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为 0，这样的 初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。</p>

<p>如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非 0 的数，结果也是一样的。</p>

<p>我们通常初始参数为正负 ε 之间的随机值，假设我们要随机初始一个尺寸为 10×11 的参数矩阵，代码如下：</p>

<p><code>
import numpy as np
a= np.random.rand(10,11) # 机初始一个尺寸为 10×11 的参数矩阵
print(a)
</code></p>

<h4 id="section-11">小结:</h4>

<ol>
  <li>参数的随机初始化</li>
  <li>利用正向传播方法计算所有的 hθ(x)</li>
  <li>编写计算代价函数 J 的代码</li>
  <li>利用反向传播方法计算所有偏导数</li>
  <li>利用数值检验方法检验这些偏导数</li>
  <li>使用优化算法来最小化代价函数</li>
</ol>

<h2 id="section-12">神经网络优化</h2>

<p>当我们建立一个神经网络学习模型之后，如何检验他到底好不好用？ 如果不好用，该怎样优化？</p>

<h4 id="section-13">假设检验</h4>

<p>为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。</p>

<ul>
  <li>线性回归模型，我们利用测试数据集计算代价函数J ；</li>
</ul>

<script type="math/tex; mode=display">
J_{test}(\theta)=\frac{1}{2m_{test}}\sum^{m_{test}}_{i=1}({h_\theta(x^{(i)}_{test})-y^{(i)}_{test}})^2
</script>

<ul>
  <li>逻辑回归模型，我们可以利用测试数据集来计算代价函数：</li>
</ul>

<script type="math/tex; mode=display">
J_{test}(\theta)=-\frac{1}{m_{test}}\sum^{m_{test}}_{i=1}(y^{(i)}_{test}\log{h_\theta(x^{(i)}_{test})}+(1-y^{(i)}_{test}）\log{(1-h_\theta(x^{(i)}_{test}))})
</script>

<h4 id="section-14">模型选择与训练集</h4>

<p>显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。</p>

<p>交叉验证集：训练集（train）用60%的数据，交叉验证集（validation）用20%的数据，测试集(test)用20%的数据</p>

<p>模型选择的方法为：</p>

<ol>
  <li>使用训练集训练出 10 个模型</li>
  <li>用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</li>
  <li>选取代价函数值最小的模型</li>
  <li>用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值）</li>
</ol>

<h4 id="section-15">诊断偏差和方差</h4>

<p>诊断偏差或是方差，即判断欠拟合还是过拟合;</p>

<ul>
  <li>训练集误差和交叉验证集误差都很高时：高偏差(欠拟合)</li>
  <li>训练集误差很小, 且交叉验证集误差远大于训练集误差时：高方差(过拟合)</li>
</ul>

<h4 id="section-16">正则化，偏差和方差</h4>

<p>选择 𝜆 的方法为：</p>

<ol>
  <li>使用训练集训练出 12 个不同程度正则化的模型</li>
  <li>用 12 模型分别对交叉验证集计算的出交叉验证误差</li>
  <li>选择得出交叉验证误差最小的模型</li>
  <li>运用步骤 3 中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与 𝜆 的值绘制在一张图表上：</li>
</ol>

<ul>
  <li>当 𝜆 较小时，训练集误差较小（过拟合）而交叉验证集误差较大</li>
  <li>随着 𝜆 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</li>
</ul>

<h4 id="section-17">学习曲线</h4>

<p>学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表。</p>

<p>思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。</p>

<p>当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。</p>

<h4 id="section-18">小结</h4>

<ul>
  <li>获得更多的训练实例——解决高方差</li>
  <li>尝试减少特征的数量——解决高方差</li>
  <li>尝试获得更多的特征——解决高偏差</li>
  <li>尝试增加多项式特征——解决高偏差</li>
  <li>尝试减少正则化程度 λ——解决高偏差</li>
  <li>
    <p>尝试增加正则化程度 λ——解决高方差</p>
  </li>
  <li>使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小</li>
  <li>使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[吴恩达机器学习笔记-2]]></title>
    <link href="https://happy123.me/blog/2019/09/11/wu-en-da-ji-qi-xue-xi-2/"/>
    <updated>2019-09-11T10:53:38+08:00</updated>
    <id>https://happy123.me/blog/2019/09/11/wu-en-da-ji-qi-xue-xi-2</id>
    <content type="html"><![CDATA[<p class="info">Logistic回归， 正则化</p>

<!-- more -->

<h2 id="logistic">1-Logistic回归</h2>

<h4 id="section">分类</h4>

<p>逻辑回归 (Logistic Regression)是分类问题的一个代表算法，这是目前最流行使用最广泛的一种学习算法。</p>

<p>我们将因变量(dependant variable)可能属于的两个类分别称为负向类（negative class）和 正向类（positive class），则因变量  𝑦∈0,1 ，其中 0 表示负向类，1 表示正向类。</p>

<p>分类问题下，可以采用逻辑回归的分类算法，这个算法的性质是：它的输出值永远在 0 到 1 之间。 它适用于标签 y 取值离散的情况，如：1 0 0 1。</p>

<h4 id="section-1">假设陈述</h4>

<p>分类问题，希望分类器的输出值在 0 和 1 之间，因此，假设函数需要满足预测值要在 0 和 1 之间。</p>

<p>回归模型的假设是：</p>

<script type="math/tex; mode=display">
h_\theta(x)=g(\theta^TX)
</script>

<p>其中：</p>

<ul>
  <li>
    <p>X 代表特征向量</p>
  </li>
  <li>
    <p>g 代表逻辑函数（logistic function）, 是一个常用的逻辑函数为 S形函数（Sigmoid function），公式为：</p>
  </li>
</ul>

<script type="math/tex; mode=display">
g(z) = \frac{1}{1+e^{-z}}
</script>

<ul>
  <li>python 代码实现sigmoid函数：</li>
</ul>

<p><code>
import numpy as np
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
</code></p>

<p>结合起来，获得逻辑回归的假设：</p>

<script type="math/tex; mode=display">
h_\theta(x) =  \frac{1}{1+e^{-\theta^TX}}
</script>

<table>
  <tbody>
    <tr>
      <td>𝜃(𝑥) 的作用是，对于给定的输入变量，根据选择的参数计算输出变量为1 的可能性 （estimated probablity），即  ℎ𝜃(𝑥)=𝑃(𝑦=1</td>
      <td>𝑥;𝜃) 。</td>
    </tr>
  </tbody>
</table>

<h4 id="section-2">代价函数</h4>

<p>逻辑回归的代价函数为：</p>

<script type="math/tex; mode=display">
J(\theta)= \frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}), y^{(i)})
</script>

<p>其中:</p>

<script type="math/tex; mode=display">
Cost(h_\theta(x), y)=-y\times{log(h_\theta(x))}-(1-y)\times{log(1-h_\theta(x))}
</script>

<p>代入代价函数:</p>

<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{m}\sum^m_{i=1}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]
</script>

<ul>
  <li>逻辑回归代价函数的Python代码实现：</li>
</ul>

<p><code>
import numpy as np
def cost(theta, X, y):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    return np.sum(first - second) / (len(X))
</code></p>

<h4 id="section-3">简化代价函数和梯度下降</h4>

<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
</script>

<p>这个更新规则和之前用来做线性回归梯度下降的式子是一样的， 但是假设的定义发生了变化。即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。</p>

<h4 id="section-4">多分类任务 一对多</h4>

<p>邮件归类， 假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，就有了一个四分类问题：其类别有四个，分别用 y=1、y=2、y=3、y=4 来代表。</p>

<p>多分类的关键就是构建多个逻辑分类函数；具体：</p>

<p>我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作 ℎ(1)𝜃(𝑥)。接着，类似地第我们选择另一个类标记为 正向类（y=2），再将其它类都标记为负向类，将这个模型记作  ℎ(2)𝜃(𝑥) ,依此类推。 最后我们得到一系列的模型简记为：</p>

<script type="math/tex; mode=display">
h^{(i)_\theta(x)} = p(y=i|x;\theta)
</script>

<p>最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器： ℎ(𝑖)𝜃(𝑥) ， 其中 i对应每一个可能的y=i，最后，为了做出预测，我们给出输入一个新的 x 值做预测。我们要做的就是在我们三个分类器里面输入 x，然后我们选择一个让  ℎ(𝑖)𝜃(𝑥) 最大的 i，即</p>

<script type="math/tex; mode=display">
\max_ih^{(i)_\theta(x)}
</script>

<h2 id="section-5">2-正则化</h2>

<h4 id="section-6">过拟合问题</h4>

<p>就以多项式理解，x 的次数越高，拟合的越好，但相应的预测的能力就可能变差。</p>

<p>如何解决？</p>

<ul>
  <li>
    <p>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如 PCA, LDA），缺点是丢弃特征的同时，也丢弃了这些相应的信息；</p>
  </li>
  <li>
    <p>正则化。 保留所有的特征，但是减少参数的大小（magnitude），当我们有大量的特征，每个特征都对目标值有一点贡献的时候，比较有效。</p>
  </li>
  <li>
    <p>还有一个解决方式就是增加数据集,因为过拟合导致的原因就过度拟合测试数据集, 那么增加数据集就很大程度提高了泛化性了.</p>
  </li>
</ul>

<h4 id="section-7">代价函数</h4>

<p>正则化的基本方法：对高次项添加惩罚值，让高次项的系数接近于0。</p>

<p>假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：</p>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m} [ \sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2} + \lambda\sum_{j=1}^n\theta^2_j  ]
</script>

<p>其中 𝜆 又称为正则化参数（Regularization Parameter）</p>

<p>```
import numpy as np
def mseWithRegular(predict, y, w, lmd=0.1):
    ‘’’
        predict: 模型输出
        y: 真实标签
        w: 模型权重
        lmd: 正则化参数
    ‘’’
    constrct_loss = np.sum((predict - y) ** 2)
    experience_loss = lmd * np.sum(w ** 2)
    loss = (constrct_loss + experience_loss) / (2 * len(predict))
    return loss</p>

<p>predict = np.array([1, 1.5, 2])
y = np.array([0.9, 1.4, 2.1])
w = np.array([[1], [1], [1]])
mseWithRegular(predict, y, w)
```</p>

<p>如果选择的正则化参数 𝜆 过大，则会把所有的参数都最小化了，导致模型变成  ℎ𝜃(𝑥)=𝜃0 ，造成欠拟合。</p>

<p>所以对于正则化，我们要取一个合理的λ的值，这样才能更好的应用正则化。</p>

<h4 id="section-8">线性回归正则化</h4>
<p>对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程</p>

<p>正则化线性回归的代价函数为：</p>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m} [ \sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2} + \lambda\sum_{j=1}^n\theta^2_j ]
</script>

<ul>
  <li>梯度下降使代价函数最小化</li>
</ul>

<script type="math/tex; mode=display">
\theta_j := \theta_j (1-a\frac{\lambda}{m})- \alpha \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
</script>

<ul>
  <li>正规方程来求解正则化线性回归模型</li>
</ul>

<p>TODO: 暂时没有理解</p>

<h4 id="section-9">逻辑回归正则化</h4>

<p>针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：梯度下降法，更高级的优化算法需要你自己设计代价函数 𝐽(𝜃) 。</p>

<p>给代价函数增加一个正则化的表达式，得到代价函数:</p>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{m}\sum^m_{i=1}[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)}log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta^2_j
</script>

<p>代码实现:</p>

<p>```
import numpy as np
def sigmoid(x, derivative=False):
    sigm = 1. / (1. + np.exp(-x))
    if derivative:
        return sigm * (1. - sigm)
    return sigm</p>

<p>def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))
    return np.sum(first - second) / (len(X)) + reg
```</p>

<p>最后，它的梯度下降看上去同正则化的线性回归一样，但是由于假设ℎ𝜃(𝑥)=𝑔(𝜃𝑇𝑋) ，所以与线性回归不同。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unicode String Parse With Python and Fileinput]]></title>
    <link href="https://happy123.me/blog/2019/09/06/unicode-string-parse-with-python-and-fileinput/"/>
    <updated>2019-09-06T11:41:03+08:00</updated>
    <id>https://happy123.me/blog/2019/09/06/unicode-string-parse-with-python-and-fileinput</id>
    <content type="html"><![CDATA[<p>用fileinput模块parse数据很方便:</p>

<p>```
import fileinput</p>

<p>if <strong>name</strong> == ‘<strong>main</strong>’:
    for line in fileinput.input():
        sys.stdout.write(line)</p>

<p>```</p>

<p>但有时候会碰到UnicodeDecodeError:</p>

<p>比如执行:</p>

<!-- more -->

<p>```
echo -e “foo\x80bar” |python3 testinput.py</p>

<p>…
UnicodeDecodeError: ‘utf8’ codec can’t decode byte 0x80 in position 3: invalid start byte
```</p>

<p>这种错误还不好用<code>try .. catch</code>忽略掉，因为它是在fileinput模块中自己parse的；</p>

<p>Python2的时候很罗嗦，需要自己用codecs去判断之后，才能parse;</p>

<p>Python3总算是引入了一个openhook参数，可以自己hook处理了；</p>

<p>最简单的处理方式:</p>

<p>```
import fileinput
import io
import sys</p>

<p>if <strong>name</strong> == ‘<strong>main</strong>’:
    sys.stdin = io.TextIOWrapper(sys.stdin.buffer, errors=’replace’)
    for line in fileinput.input(openhook=fileinput.hook_encoded(“utf-8”)):
        sys.stdout.write(line)</p>

<p>```</p>

<p>参考:</p>

<p>https://stackoverflow.com/questions/24754861/unicode-file-with-python-and-fileinput</p>

<p>https://bugs.python.org/issue26756</p>
]]></content>
  </entry>
  
</feed>
