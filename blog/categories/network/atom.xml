<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Network | Living a Simple Life is a Happy Life]]></title>
  <link href="http://happy123.me/blog/categories/network/atom.xml" rel="self"/>
  <link href="http://happy123.me/"/>
  <updated>2018-09-10T12:16:42+08:00</updated>
  <id>http://happy123.me/</id>
  <author>
    <name><![CDATA[memoryboxes]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[为什么不用MAC地址来定位机器]]></title>
    <link href="http://happy123.me/blog/2018/09/09/wei-shi-yao-bu-yong-macdi-zhi-lai-ding-wei-ji-qi/"/>
    <updated>2018-09-09T15:09:42+08:00</updated>
    <id>http://happy123.me/blog/2018/09/09/wei-shi-yao-bu-yong-macdi-zhi-lai-ding-wei-ji-qi</id>
    <content type="html"><![CDATA[<p>在知乎看到了这个问题，MAC地址48Bit, IP地址32Bit，完全可以用MAC划分出一个段来作为IP的映射，或者干脆不用IP了，有啥不可以呢？</p>

<p>我觉得问题挺有意思的。我推测了一下。</p>

<p>这纯粹是一个历史演变：</p>

<p>10Base 以太网[Ethernet Version 2（EV2）] 是由施乐公司出去的人发明的，后来以太网被市场承认了就纳入IEEE802标准，这是20世纪80年代的事情，那个时候就已经定义MAC地址了。</p>

<p>TCP/IP虽然最初是在1983年提出的，但是发展还是在1990年之后；另外TCP/IP也不是一个协议，而是一组协议簇，像ARP之类的和MAC地址相关的协议，很明显的，是在已有的硬件基础上开发的。</p>

<p>所以结论很明显，在早期只有局域网的时代，诞生了MAC地址这种硬件地址规定；后来联网机器越来越多，TCP/IP 成为主流联网协议，但是它的年龄和MAC地址是差不多的，也不可能再重新搞，为了适配现有的局域网模式，开发了ARP等协议；</p>

<p>计算机很多问题都是工程问题、商业问题，并不是技术完美就通吃天下，很多东西都是历史积淀、为了兼容现有系统而发展出来的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP/IP Illustrated Note1]]></title>
    <link href="http://happy123.me/blog/2016/06/05/tcp-slash-ip-illustrated-note1/"/>
    <updated>2016-06-05T21:59:52+08:00</updated>
    <id>http://happy123.me/blog/2016/06/05/tcp-slash-ip-illustrated-note1</id>
    <content type="html"><![CDATA[<p>又开始重读&lt;TCP/IP详解>。发现真的是年纪大了&hellip;.T_T，看了就忘，做笔记也用处不大。</p>

<p>打算每天读完一部分，在这里捡一些重要的地方记一下。</p>

<h2>ARP</h2>

<p>习得技能:</p>

<ul>
<li><code>tcpdump -e</code> 可以显示硬件层地址</li>
</ul>


<h2>IP</h2>

<p>重要提示:</p>

<ul>
<li>传输之前需要把首部转换成网络字节序。注意，仅仅是首部就可以。</li>
</ul>


<h2>ICMP</h2>

<p>重要提示:</p>

<ul>
<li><p>在对ICMP差错报文响应时，永远不会生成另一份ICMP差错报文</p></li>
<li><p>当发放一份ICMP差错报文时，始终包含IP首部和产生ICMP差错报文的IP数据包的前8个字节。这样就可以精准知道谁产生的ICMP差错</p></li>
<li><p>ICMP的典型应用:</p>

<ul>
<li><p>掩码请求和应答 (大多数主机在收到请求后都会产生应答，而不是指定的主机应答，这其实是实现上的冗余)</p></li>
<li><p>时间戳请求和应答</p></li>
</ul>
</li>
<li><p>多宿主机发送报文给自己的某个接口，其实都是给回环地址的，这样产生的ICMP应答其实搞不明白原始报文发给哪个接口</p></li>
</ul>


<h2>Ping</h2>

<p>习得技能:</p>

<ul>
<li><p><code>Ping -S</code> 每秒一个往返请求</p></li>
<li><p><code>Ping -R</code> 记录路由，但是有限制，最大只能存放9个IP地址，而且只记录路由出口</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[再谈以太网帧格式]]></title>
    <link href="http://happy123.me/blog/2016/05/23/zai-tan-yi-tai-wang-zheng-ge-shi/"/>
    <updated>2016-05-23T21:59:43+08:00</updated>
    <id>http://happy123.me/blog/2016/05/23/zai-tan-yi-tai-wang-zheng-ge-shi</id>
    <content type="html"><![CDATA[<p>又重新读了一遍《tcp/ip详解》，又重温了一遍万年知识以太网，为了不能忘却的回忆，我决定原文摘抄一遍。</p>

<p>以太网这个术语是指DEC、Intel和Xerox公司在1982年联合公布的一个标准。它是当今TCP/IP采用的主要局域网技术。
几年后，IEEE802委员会公布了一个稍有不同的标准集，其中802.3针对整个CSMA/CD网络，802.4针对令牌总线网络，
802.5针对令牌环网络，这三者的共同特性由802.2标准来定义，那就是802网络共有的逻辑链路控制（LLC）。不幸的
是，802.3定义了一个与以太网不同的帧格式。</p>

<p>下图定义了两种不同形式的封装格式：</p>

<p><img src="https://raw.githubusercontent.com/memoryboxes/memoryboxes.github.io/source/images/802_3.jpg" alt="802.3" /></p>

<p>在以太网帧格式中，类型字段之后就是数据，而在802帧格式中，跟随在后面的3字节的802.2LLC和5字节的802.2SNAP。</p>

<p>目的服务访问（DSAP）和源服务访问点（SSAP）的值都设为0xaa。ctrl字段的值设为3.随后的3个字节org code都设置为0。</p>

<p>再接下来的2个字节类型字段和以以太网帧格式一样。</p>

<p>CRC字段用于帧内后续字节差错的循环冗余码检验。</p>

<p>802.3标准定义的帧和以太网的帧都有最小长度要求。802.3规定数据部分必须至少为38字节，而对于以太网，则要求最少要有46字节。为了保证这一点，必须在不足的空间插入填充字节。</p>

<p>最后注意一下，mtu的大小只是指帧内容的大小，不包括帧头。所以mtu是1500时，QinQ又额外的四个字节，加上帧头共1522个字节，会丢包，最小要把mtu设置为1504。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Udp Packet Receive Errors]]></title>
    <link href="http://happy123.me/blog/2016/05/20/udp-packet-receive-errors/"/>
    <updated>2016-05-20T08:09:35+08:00</updated>
    <id>http://happy123.me/blog/2016/05/20/udp-packet-receive-errors</id>
    <content type="html"><![CDATA[<h2>Issue</h2>

<p>netstat -s output shows high number of Udp: packet receive errors</p>

<p>Getting high number of UDP packet drops or loss</p>

<p>SNMP trap issue :&ndash; SNMP trap seems to be fluctuating on my RHEL server.</p>

<h2>Resolution</h2>

<p>Udp: packet receive errors is increased for the following reasons:</p>

<ul>
<li><p>Not enough socket buffer space</p></li>
<li><p>UDP checksum failure</p></li>
<li><p>UDP length mismatch</p></li>
<li><p>IPSec Security Policy failure</p></li>
</ul>


<h2>Diagnostic Steps</h2>

<h4>Gather statistics</h4>

<p>Run the command netstat -nsu and see the Udp: section:</p>

<pre><code>netstat -su

Udp:
    559933412 packets received
    71 packets to unknown port received.
    33861296 packet receive errors    &lt;---- HERE
    7516291 packets sent
Socket buffer checking:
</code></pre>

<p>The current system-wide default socket buffer size can be determined with the commands:</p>

<pre><code>sysctl net.core.rmem_max
sysctl net.core.rmem_default
</code></pre>

<p>This can be confirmed by watching socket statistics whilst packet receive errors is growing by running ss -nump at regular intervals, for example:</p>

<pre><code>while true; do ss -nump; sleep 1; done
</code></pre>

<p>This will produce output as follows:</p>

<pre><code>State    Recv-Q Send-Q    Local Address:Port      Peer Address:Port
ESTAB    0      0           192.168.0.2:4500       192.168.0.1:4500
users:(("processname",pid=1234,fd=3))
        skmem:(r0,rb212992,t0,tb212992,f4096,w0,o0,bl0)
</code></pre>

<p>If the Recv-Q statistic is regularly growing large, such as approading the system-wide rmem_max, then increase the socket buffer size.</p>

<p>Note this means the application is not receiving from the buffer fast enough. It may be necessary to reconfigure or redesign the application to perform better.</p>

<h2>Conclusion</h2>

<p>The statistic Udp: packet receive errors is reporting the SNMP MIB called UDP_MIB_INERRORS.</p>

<h2>Commands</h2>

<ul>
<li><p>run udp server</p>

<pre><code>  nc -4 -u -l 2389
</code></pre></li>
<li><p>cli to server</p>

<pre><code>  echo -n "hello" | nc -u x.x.x.x 2389
</code></pre></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open vSwitch Bridge and NetworkNameSpace Command Cheat]]></title>
    <link href="http://happy123.me/blog/2016/05/11/open-vswitch-bridge-and-networknamespace-command-cheat/"/>
    <updated>2016-05-11T15:41:02+08:00</updated>
    <id>http://happy123.me/blog/2016/05/11/open-vswitch-bridge-and-networknamespace-command-cheat</id>
    <content type="html"><![CDATA[<ul>
<li><p>启动</p>

<pre><code>  service openvswitch start
</code></pre></li>
<li><p>创建网桥</p>

<pre><code>  ovs-vsctl add-br br0
  ifconfig br0 up
</code></pre></li>
<li><p>显示所有网桥</p>

<pre><code>  ovs-vsctl show
</code></pre></li>
<li><p>删除网桥</p>

<pre><code>  ovs-vsctl del-br br0
</code></pre></li>
<li><p>增加端口</p>

<pre><code>  ovs-vsctl add-port br0 eth0
</code></pre></li>
<li><p>设置网卡为none</p>

<pre><code>  dhclient br0
</code></pre></li>
<li><p>用 Namespace 模拟两台虚拟机网络</p>

<pre><code>  p netns add network1
  ip netns add network2
</code></pre></li>
<li><p>创建两个虚拟网卡并加入网桥</p>

<pre><code>  ovs-vsctl add-port br0 vport1 -- set interface vport1 type=internal
  ovs-vsctl add-port br0 vport2 -- set interface vport2 type=internal
  tunctl -p -t vport1
  tunctl -p -t vport2
</code></pre></li>
<li><p>两个虚拟网卡接入namespace</p>

<pre><code>  ip link set vport1 netns network1
  ip link set vport2 netns network2
</code></pre></li>
<li><p>设置虚拟网卡的IP</p>

<pre><code>  ip netns exec network1 ifconfig vport1 192.168.0.1/24 up
  ip netns exec network2 ifconfig vport2 192.168.0.2/24 up
</code></pre></li>
<li><p>两个namsespace PING</p>

<pre><code>  ip netns exec network1 ping 192.168.0.2
  ip netns exec network2 tcpdump -i vport2
</code></pre></li>
<li><p>两个namsespace  NC传输</p>

<pre><code>  ip netns exec network2 nc -l 1234
  ip netns exec network2 tcpdump -i vport2
  ip netns exec network1 nc 192.168.0.2 1234
</code></pre></li>
<li><p>显示vlan信息</p>

<pre><code>  ovs-appctl fdb/show br0
</code></pre></li>
<li><p>显示openflow信息</p>

<pre><code>  ovs-ofctl show br0
</code></pre></li>
<li><p>显示流表信息</p>

<pre><code>  ovs-ofctl dump-flows br0
</code></pre></li>
<li><p>显示网桥详细信息</p>

<pre><code>  ovs-vsctl list Bridge
</code></pre></li>
<li><p>显示端口详细信息</p>

<pre><code>  ovs-vsctl list Port
</code></pre></li>
<li><p>显示接口详细信息</p>

<pre><code>  ovs-vsctl list Interface
</code></pre></li>
</ul>

]]></content>
  </entry>
  
</feed>
