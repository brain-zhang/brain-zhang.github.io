<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tools | Living a Simple Life is a Happy Life]]></title>
  <link href="http://happy123.me/blog/categories/tools/atom.xml" rel="self"/>
  <link href="http://happy123.me/"/>
  <updated>2018-05-04T08:24:09+08:00</updated>
  <id>http://happy123.me/</id>
  <author>
    <name><![CDATA[memoryboxes]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Sort a Very Very Very Big File]]></title>
    <link href="http://happy123.me/blog/2018/05/03/how-to-sort-a-very-very-very-big-file/"/>
    <updated>2018-05-03T07:21:50+08:00</updated>
    <id>http://happy123.me/blog/2018/05/03/how-to-sort-a-very-very-very-big-file</id>
    <content type="html"><![CDATA[<p>sort -uo 一个1T的文件，让最高配的google cloud instance (48 core/512G)崩溃了<del>，可惜了我的$30，白白跑了那么长时间</del></p>

<p>网上搜索都是how to sort a big file，那我这个属于very very very big big big file了~~</p>

<p>不管是并行也好，管道也好，用了各种奇技淫巧就是敌不过人家 very very big~</p>

<p>不要跟我谈什么外排，归并，位图，bloom filter，redis hash去重，我就是不想折腾，最后只有分割搞定~~</p>

<h3>把大象装进冰箱分为几步？</h3>

<h3>三步:</h3>

<ol>
<li><p>split -l 1000000000 huge-file small-chunk</p></li>
<li><p>for X in small-chunk*; do sort -u &lt; $X > sorted-$X; done</p></li>
<li><p>sort -u -m sorted-small-chunk<em> > sorted-huge-file &amp;&amp; rm -rf small-chunk</em> sorted-small-chunk*</p></li>
</ol>


<h3>小TIPS:</h3>

<p>如果只要去重不要排序的话，尽量不要用 sort -u或者sort | uniq，这个是nLog(n)的效率，让人捉急。</p>

<p>可以利用awk的数组是内存hash表的特性，直接awk来做:</p>

<p><code>
cat xxxxx zzz | awk '{ if (!seen[$0]++) { print $0; } }' &gt; xxx_zzz.uniq.txt
</code></p>

<h3>PS:</h3>

<p>我后来又看了一下GNU Sort的实现描述，它说已经用了外排了，但是实际使用还是不给力，暂时迷惑中</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Get Intersection of Two Big Files]]></title>
    <link href="http://happy123.me/blog/2018/05/01/how-to-get-intersection-of-two-big-files/"/>
    <updated>2018-05-01T22:18:33+08:00</updated>
    <id>http://happy123.me/blog/2018/05/01/how-to-get-intersection-of-two-big-files</id>
    <content type="html"><![CDATA[<p>两个大文件，a.txt和b.txt两个文件的数据都是逐行呈现的， 如何求他们的交集、并集和差集。</p>

<p>用sort+uniq直接搞定:</p>

<h2>交集</h2>

<p><code>
$ sort a.txt | uniq &gt; aa.txt
$ sort b.txt | uniq &gt; bb.txt
$ cat aa.txt bb.txt | sort | uniq -d
</code></p>

<h2>并集</h2>

<p><code>
cat a.txt b.txt | sort | uniq
</code></p>

<h2>差集</h2>

<p><code>
$ sort a.txt | uniq &gt; aa.txt
$ sort b.txt | uniq &gt; bb.txt
$ cat aa.txt bb.txt bb.txt | sort | uniq -u
</code></p>

<ul>
<li>在开搞 bloom filter或者bitmap 或者grep -f之前可以先组合工具来一个</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Parallel All Cmds for Linux]]></title>
    <link href="http://happy123.me/blog/2018/05/01/how-to-parallel-all-cmds-for-linux/"/>
    <updated>2018-05-01T13:07:20+08:00</updated>
    <id>http://happy123.me/blog/2018/05/01/how-to-parallel-all-cmds-for-linux</id>
    <content type="html"><![CDATA[<p>grep 一个100GB的文件总是很有压力，怎么才能提速呢?</p>

<h3>瞎优化</h3>

<p><code>
LC_ALL=C fgrep -A 5 -B 5 'xxxxx.password' allpassseed.txt
</code></p>

<ul>
<li><p><code>LC_ALL=C</code>比<code>LC_ALL=UTF-8</code>要块</p></li>
<li><p>不需要正则的话，用fgrep可以提速</p></li>
</ul>


<h3>不过这样优化总是治标不治本，下面隆重推出linux 里面parallel all cmds的perl工具</h3>

<p><code>
cat allpassseed.txt |parallel  --pipe  --no-notice grep -f xxxxx.password
</code></p>

<p>使用parallel ，和不使用parallel直接grep。结果显而易见，相差 20 倍。这比用啥 ack，ag优化效果明显多了</p>

<h3>xargs也有一个-n的多核选项，可以作为备用</h3>

<p>```
$ time echo {1..5} |xargs -n 1  sleep</p>

<p>real    0m15.005s
user    0m0.000s
sys 0m0.000s
```</p>

<p>这一条xargs把每个echo的数作为参数传给sleep ，所以一共sleep了 1+2+3+4+5=15秒。</p>

<p>如果使用 -P 参数分给5个核，每个核各sleep 1,2,3,4,5秒，所以执行完之后总共sleep的5秒。</p>

<p>```
$ time echo {1..5} |xargs -n 1 -P 5 sleep</p>

<p>real    0m5.003s
user    0m0.000s
sys 0m0.000s
```</p>

<ul>
<li>引自:</li>
</ul>


<p><a href="https://www.jianshu.com/p/c5a2369fa613">https://www.jianshu.com/p/c5a2369fa613</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Calling Multiple Commands Through Xargs]]></title>
    <link href="http://happy123.me/blog/2018/05/01/how-to-calling-multiple-commands-through-xargs/"/>
    <updated>2018-05-01T09:25:03+08:00</updated>
    <id>http://happy123.me/blog/2018/05/01/how-to-calling-multiple-commands-through-xargs</id>
    <content type="html"><![CDATA[<p>有时候想在xargs后面接多条命令，这个时候直接加<code>;</code>是不行的，要这样做:</p>

<p><code>
cat a.txt | xargs -I@  sh -c 'command1; command2; ...'
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Compress All Find Files to Single Line Argv]]></title>
    <link href="http://happy123.me/blog/2018/04/30/how-to-compress-all-find-files-to-single-line-argv/"/>
    <updated>2018-04-30T10:39:39+08:00</updated>
    <id>http://happy123.me/blog/2018/04/30/how-to-compress-all-find-files-to-single-line-argv</id>
    <content type="html"><![CDATA[<p>有时候find的所有文件要合并为一个argv管道到一个命令里面:</p>

<p><code>
find /path/to/directory/ -name *.csv -print0 | xargs -0 -I file cat file &gt; merged.file
</code></p>
]]></content>
  </entry>
  
</feed>
