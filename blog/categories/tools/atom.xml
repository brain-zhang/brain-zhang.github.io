<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tools | Living a Simple Life is a Happy Life]]></title>
  <link href="https://brain-zhang.github.io/blog/categories/tools/atom.xml" rel="self"/>
  <link href="https://brain-zhang.github.io/"/>
  <updated>2023-01-24T17:07:01+08:00</updated>
  <id>https://brain-zhang.github.io/</id>
  <author>
    <name><![CDATA[brain-zhang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Migrate PVE Storage From ZFS to Lvm]]></title>
    <link href="https://brain-zhang.github.io/blog/2023/01/18/migrate-pve-storage-zfs-to-lvm/"/>
    <updated>2023-01-18T10:21:13+08:00</updated>
    <id>https://brain-zhang.github.io/blog/2023/01/18/migrate-pve-storage-zfs-to-lvm</id>
    <content type="html"><![CDATA[<p>之前PVE的存储我用ZFS做了三个pool，一个RaidZ1，两个Raid0；</p>

<p>我的PVE内核版本为 <code>5.13.19-6-pve</code>;</p>

<p>实际用下来有一个大槽点；</p>

<p>就是对于12T以上的单盘，ZFS每次写入的时候都会炒豆子音大爆发，放在家里实在不是什么好体验；而且我一直有个困扰已久的问题没有解决；就是ZFS强壮是强壮；但是对于其dataset的管理方式，无论是send还是destroy，每个操作都会有长时间的卡顿lock；另外相对于读性能，不知道为什么，我的RaidZ1写性能一直没有达到单盘读写能力，我加了ARC、ZIL，各种方法都折腾了一遍，但是效果都不好；
还有一个最大的问题，就是虚拟机多了之后，比如我同时对20台虚拟机做硬盘迁移操作；ZFS的lock太严重了，比如我同时delete两个虚拟硬盘，必定lock timeout;这个lock timeout 60s的限制，没有找到设置的地方，只能硬改代码，非常tricky；而不用后台命令，PVE的web UI对于并行操作的支持不好，所以降低磁盘并行操作的locktime非常必要；</p>

<!-- more -->

<p>当然实际使用上，ZFS的优点也很突出:</p>

<ul>
  <li>透明压缩、文件去重；太有用了，尤其是PVE 创建LXC容器，文件存储直接继承ZFS的所有能力</li>
  <li>dataset级别的存储操作；这种块级别管理文件的方式，对于大规模数据迁移很有用，而且可以针对不同的需求对每一个dataset参数调整</li>
  <li>快照；ZFS的杀手特性</li>
</ul>

<p>总之折腾了一年多，我已经把ZFS的手册翻了好几遍了，我已经理解这个文件系统的使用方式了；</p>

<p>但是为了静音，我打算切换到PVE 源远流长的LVM存储管理；鉴于现在我的忘性越来越大，我想用尽可能简单的方式，来描述PVE如何使用LVM的；以后随时能扫一眼回忆回忆；</p>

<h2 id="lvm">几句话过一下LVM</h2>

<ul>
  <li>LVM是整个一套管理磁盘存储的机制；包括分区扩容缩减，替换磁盘等等等等</li>
  <li>一个物理磁盘称之为PhysicalStorageMedia</li>
  <li>LVM最底层的设备称之为PV(物理卷)， 物理卷可以是一组raid盘，可以是单个物理硬盘，可以是一个分区(比如/dev/sda1)</li>
  <li>多个PV组成了VG(卷组)，一个VG对外表现就是一个块设备，就像一块硬盘一样</li>
  <li>一个VG可以划分为多个LV(逻辑卷)；其表现就跟一块硬盘划分多个分区是一样的</li>
  <li>有一种特殊的LV，称之为Thinly-Provisioned Logical Volumes(精简模式LVM)；</li>
  <li>thin LV支持COW(快照方便)和动态存储分配空间(按需分配而不是虚拟之指定的磁盘大小，节约空间)，跟ZFS一样，适合云环境</li>
  <li>创建thinLV之前，必须先创建一个thinpool，次序依次是 创建PV-&gt;创建VG-&gt;在此VG上创建thinpool-&gt;在此thinpool上创建thin LV；</li>
  <li>LVM可以动态缩减空间，增删硬盘</li>
  <li>一个VG可以单个PV，也可以多个PV组成</li>
  <li>一个VG可以包含多个thinpool+多个普通LV</li>
  <li>VG可以动态扩展，空间可以动态调整</li>
  <li>LV空间可以动态调整</li>
</ul>

<h2 id="pvelvm">PVE中的LVM</h2>

<ul>
  <li>PVE上后台用命令行可以支持所有LVM特性</li>
  <li>PVE Web界面功能比较弱，只支持
    <ul>
      <li>将一个PV划分为一个VG</li>
      <li>将一个VG划分为一个thinpool，即lvm-thin</li>
      <li>Web UI不可以组合划分</li>
    </ul>
  </li>
</ul>

<p>参考了一篇非常详细的文章，人家写的很清楚，就不啰嗦了:</p>

<p>https://codeantenna.com/a/SG6LHk1x9s</p>

<h2 id="section">规划</h2>

<ul>
  <li>三块硬盘，分成三个VG</li>
  <li>两个VG做成thin pool，只有lvm thin，分别用于存储LXC容器和VM</li>
  <li>一个VG 用来做文件服务器</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rsync Cheat]]></title>
    <link href="https://brain-zhang.github.io/blog/2022/07/26/rsync-cheat/"/>
    <updated>2022-07-26T15:35:51+08:00</updated>
    <id>https://brain-zhang.github.io/blog/2022/07/26/rsync-cheat</id>
    <content type="html"><![CDATA[<p>这条命令我用了不下百次了，但是每次还得查，老年痴呆的前兆，/(ㄒoㄒ)/~~</p>

<p><code>
rsync -acvruP --progress /opt/src1 /opt/src2  /mnt/dest/
</code></p>

<ul>
  <li>-c, –checksum 打开校验开关，强制对文件传输进行校验; 开了前置计算会很慢，文件多的时候不要用</li>
  <li>-a, –archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD</li>
  <li>-r, –recursive 对子目录以递归模式处理</li>
  <li>-u, –update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件。(不覆盖更新的文件)；已经有拷贝了一半的文件的情况下不要用，不会自动更新</li>
</ul>

<p>最后，要注意路径中的斜杠处理:</p>

<p><code>
rsync -acvruP --progress /opt/src1 /mnt/dest/
</code></p>

<p>与下面的命令是不一样的</p>

<p><code>
rsync -acvruP --progress /opt/src1/ /mnt/dest/
</code></p>

<p>前者会拷贝src1目录，后者会拷贝src1目录下的文件，但是不会带src1</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ZFS Cheat]]></title>
    <link href="https://brain-zhang.github.io/blog/2022/07/26/zfs-cheat/"/>
    <updated>2022-07-26T15:14:19+08:00</updated>
    <id>https://brain-zhang.github.io/blog/2022/07/26/zfs-cheat</id>
    <content type="html"><![CDATA[<h2 id="section">特别提示</h2>

<p>在各种折腾之前，先看看你买的大容量硬盘自带的缓存开了没有；有一些矿盘，不知道是何原因，默认缓存没有开</p>

<p>```
# 查看是否开了写缓存
$ hdparm -W /dev/sdx</p>

<h1 id="section-1">开启</h1>
<p>$ hdparm -W1 /dev/sdx
```</p>

<p>然后，看看你的SATA接口当前速率是2.0还是3.0，有人就是这么粗心，拿着3.0的盘，插着2.0的线；</p>

<p><code>
$ smartctl -a /dev/sdx
</code></p>

<h2 id="zfs-">ZFS 使用命令小集</h2>

<h4 id="zpool">列出zpool磁盘</h4>
<p><code>
zfs list
</code></p>

<h4 id="pool">查看pool状态</h4>
<p><code>
zpool status
</code>
<!-- more --></p>

<h4 id="section-2">替换坏掉的硬盘</h4>
<p><code>
zpool replace -f pool0 /dev/sdb
</code></p>

<h4 id="section-3">查看是否开启重复数据删除</h4>
<p><code>
zfs get dedup pool1
</code></p>

<h4 id="section-4">开启重复数据删除</h4>
<p><code>
zfs set dedup=on pool1
</code></p>

<h4 id="section-5">获取去重比例</h4>
<p>```
# zpool get dedupratio pool1</p>

<p>NAME  PROPERTY    VALUE  SOURCE
tank  dedupratio  1.42x  -
```</p>

<h4 id="section-6">查看是否开启压缩</h4>
<p><code>
zfs get compress pool1
</code></p>

<h4 id="section-7">开启压缩</h4>
<p><code>
zfs set compress=lz4 pool1
</code></p>

<p>或者</p>

<p><code>
zfs set compress=on pool1
</code></p>

<p>从2015年zfs版本后，默认压缩为lz4格式， compresss=on 即代表压缩为lz4， <a href="!http://open-zfs.org/wiki/Performance_tuning#Compression">参考</a></p>

<h4 id="section-8">获取压缩比例</h4>
<p><code>
root@ypcpve:~# zfs get compressratio zpool0
NAME    PROPERTY       VALUE  SOURCE
zpool0  compressratio  1.15x  -
</code></p>

<h4 id="section-9">强制删除不用的硬盘</h4>
<p><code>
zfs destroy -f zpool0/vm-102-disk-2
</code></p>

<h4 id="section-10">数据集迁移</h4>
<p><code>
zfs snapshot oldpool/mydataset@snapshot1
zfs send oldpool/mydataset@snapshot1 | zfs receive newpool/mydataset
zfs snapshot oldpool/mydataset@snapshot2
zfs send -i oldpool/mydataset@snapshot1 oldpool/mydataset@snapshot2 | zfs receive newpool/mydataset
</code></p>

<h4 id="sparse">开启空间自动回收机制(sparse)</h4>
<p><code>
zfs set refreservation=0G NVMe/vm-901-disk-0
</code></p>

<h4 id="section-11">查看磁盘负载</h4>
<p><code>
zpool iostat
</code></p>

<h4 id="section-12">查看磁盘状态及容量</h4>

<p>```
root@proxmox4 ~ &gt; zfs list rpool/data/vm-100-disk-1
NAME                       USED  AVAIL  REFER  MOUNTPOINT
rpool/data/vm-100-disk-1   132G   832G    64K  -</p>

<p>root@proxmox4 ~ &gt; zfs get all rpool/data/vm-100-disk-1
NAME                      PROPERTY              VALUE                 SOURCE
rpool/data/vm-100-disk-1  type                  volume                -
rpool/data/vm-100-disk-1  creation              Mi Feb 21 13:29 2018  -
rpool/data/vm-100-disk-1  used                  132G                  -
```</p>

<h4 id="arc-">查看 ARC 缓存大小</h4>

<p>```
# 得到MB
root@proxmox4 ~ &gt; awk ‘/^size/ { print $1 “ “ $3 / 1048576 }’ &lt; /proc/spl/kstat/zfs/arcstats</p>

<h1 id="section-13">查看最大设置，默认0代表使用系统的一半内存</h1>
<p>root@proxmox4 ~ &gt; cat /sys/module/zfs/parameters/zfs_arc_max
```</p>

<h2 id="pve">pve规划</h2>

<p>pve 为每个虚拟机以及容器在zpool上直接创建sub vol； 这样不利于管理；最好为每一类虚拟机单独创建一个dataset；例如:</p>

<p>```
# 为每种虚机按用途分别归类dataset</p>

<p>zfs create zpool0/linuxdateset
zfs create zpool0/windataset
zfs create zpool0/lxcdataset
zfs create zpool0/productdataset
zfs create zpool0/testdataset</p>

<h1 id="dataset">查看所有dataset</h1>
<p>pvesm zfsscan
```</p>

<p>建立这些dataset后，要到PVE的管理界面上 <code>Datacenter-&gt;Stortage</code> 添加相应的挂载点，然后把虚拟机的硬盘分门别类存放</p>

<p>这样就可以为每个dataset设置不同的属性；比如我们测试环境的数据可靠性要求比较低，我们为zpool0/testdataset 关闭同步功能，这样会大幅提升读写性能</p>

<p><code>
zfs set sync=disabled zpool0/testdataset
</code></p>

<h2 id="ssdzil">在SSD上分配ZIL缓存</h2>

<p>在SSD上创建log, read缓存，为zfs pool 机械盘加速</p>

<h4 id="section-14">建立缓存文件并挂载</h4>
<p>```
mkidr /zcache &amp;&amp; cd /zcache
fallocate -l 16G zfs-log-cache.img
fallocate -l 16G zfs-read-cache.img</p>

<p>losetup -fP zfs-log-cache.img
losetup -fP zfs-log-read.img
```</p>

<h4 id="section-15">查看挂载情况</h4>
<p>```
losetup -a</p>

<p>/dev/loop0: [66306]:37691140 (/zcache/zfs-log-cache.img)
/dev/loop1: [66306]:37691139 (/zcache/zfs-read-cache.img)
```</p>

<h4 id="zpool-1">添加到zpool</h4>
<p><code>
zpool add zpool0 log /dev/loop0
zpool add zpool0 read /dev/loop1
</code></p>

<h4 id="section-16">检查使用情况</h4>
<p><code>
watch "zpool iostat -v"
</code></p>

<h4 id="section-17">移除</h4>
<p><code>
zpool remove zpool0 /dev/loop0
zpool remove zpool0 /dev/loop1
</code></p>

<h4 id="section-18">开机自动挂载</h4>
<p>```
vim /etc/fstab</p>

<h1 id="section-19">添加</h1>
<p>/zcache/zfs-log-cache.img       /dev/loop0       ext4       loop       0 0
/zcache/zfs-read-cache.img      /dev/loop1       ext4       loop       0 0
```</p>

<h2 id="section-20">磁盘替换</h2>

<p>大容量磁盘RaidZx重建是一个极其缓慢的过程；实测我的12T X3 Raidz1阵列，替换一块坏盘重建花了三天；</p>

<p><code>
$ sudo zpool offline zpool0 ata-TOSHIBA01
$ sudo zpool online zpool0 ata-TOSHIBA02
$ sudo zpool replace ata-TOSHIBA01 ata-TOSHIBA02
</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Limit Exists Running Docker Container Cpus]]></title>
    <link href="https://brain-zhang.github.io/blog/2021/11/07/how-to-limit-docker-cpus/"/>
    <updated>2021-11-07T11:05:44+08:00</updated>
    <id>https://brain-zhang.github.io/blog/2021/11/07/how-to-limit-docker-cpus</id>
    <content type="html"><![CDATA[<p><code>
docker update --cpu-period=100000 --cpu-quota=40000  &lt;container&gt;
</code></p>

<p>意思是cpu时间切分为100000份，指定容器占用40000份，即cpu占用率最高40%</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Allow Non-root Process to Bind to Low-numbered Ports]]></title>
    <link href="https://brain-zhang.github.io/blog/2021/06/01/how-to-allow-non-root-process-to-bind-to-low-numbered-ports/"/>
    <updated>2021-06-01T16:32:31+08:00</updated>
    <id>https://brain-zhang.github.io/blog/2021/06/01/how-to-allow-non-root-process-to-bind-to-low-numbered-ports</id>
    <content type="html"><![CDATA[<h4 id="use-capnetbindservice-to-grant-low-numbered-port-access-to-a-process">Use CAP_NET_BIND_SERVICE to grant low-numbered port access to a process:</h4>

<p>With this you can grant permanent access to a specific binary to bind to low-numbered ports via the setcap command:</p>

<p><code>
sudo setcap CAP_NET_BIND_SERVICE=+eip /path/to/binary
</code></p>

<p>For more details on the e/i/p part, see cap_from_text.</p>

<p>After doing this, /path/to/binary will be able to bind to low-numbered ports. Note that you must use setcap on the binary itself rather than a symlink.</p>

<p>FROM:</p>

<p>https://superuser.com/questions/710253/allow-non-root-process-to-bind-to-port-80-and-443</p>
]]></content>
  </entry>
  
</feed>
