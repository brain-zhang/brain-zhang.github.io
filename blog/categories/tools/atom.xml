<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tools | Living a Simple Life is a Happy Life]]></title>
  <link href="http://happy123.me/blog/categories/tools/atom.xml" rel="self"/>
  <link href="http://happy123.me/"/>
  <updated>2018-06-11T10:22:29+08:00</updated>
  <id>http://happy123.me/</id>
  <author>
    <name><![CDATA[memoryboxes]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Split Large ‘tar’ Archive Into Multiple Files of Certain Size]]></title>
    <link href="http://happy123.me/blog/2018/06/10/how-to-split-large-tar-archive-into-multiple-files-of-certain-size/"/>
    <updated>2018-06-10T21:50:43+08:00</updated>
    <id>http://happy123.me/blog/2018/06/10/how-to-split-large-tar-archive-into-multiple-files-of-certain-size</id>
    <content type="html"><![CDATA[<p>有时候需要压缩文件的时候同时分割一下:</p>

<p><code>
tar czvf - -C /mnt/g/dict/ weakpass_merge.dict |split -b 10000M - "weakpass.part.tar.gz."
</code></p>

<p>还原:</p>

<p><code>
cat weakpass.part.tar.gz.*|tar zxvf
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Improve Performance Your Cmd by Parallel]]></title>
    <link href="http://happy123.me/blog/2018/05/06/how-to-improve-performance-your-cmd-by-parallel/"/>
    <updated>2018-05-06T09:32:03+08:00</updated>
    <id>http://happy123.me/blog/2018/05/06/how-to-improve-performance-your-cmd-by-parallel</id>
    <content type="html"><![CDATA[<p>有很多时候，处理一个大文件，常规命令并不能很好的利用多核</p>

<!-- more -->


<p>例如，一个1T的文本，百亿条数据，我想要:</p>

<p><code>
wc -l test.txt
</code></p>

<p>或者</p>

<p><code>
fgrep xxxx test.txt
</code></p>

<p>一般机器就会自觉进入<code>一核有难，其它核点赞</code>的看戏模式。</p>

<p>我花钱配了这么多核，加了这么多内存，不是让大家来看戏的。于是祭出<code>parallel</code>~</p>

<h2>原理</h2>

<p>parallel 是一个perl脚本，通过分割输入，并行处理的方式来加速执行命令。</p>

<p>例如:</p>

<p><code>
wc -l test.txt
</code></p>

<p>简单想想就是用个for循环split文件，挨个wc，然后相加。parallel就是自动帮你把这类事情做掉而已。大道不过两三行，所谓外部排序，Map-Reduce莫不如是。</p>

<h2>安装 (ubuntu 16.04LTS)</h2>

<p><code>
 apt-get install parallel
</code></p>

<h2>示例</h2>

<h4>最快的办法计算一个大文件的行数</h4>

<p><code>
cat bigfile.txt | parallel --no-notice --pipe wc -l | awk '{s+=$1} END {print s}'
</code></p>

<p>非常的巧妙，先使用parallel命令‘mapping’出大量的wc -l调用，形成子计算，最后通过管道发送给awk进行汇总</p>

<h4>SED, 想在一个巨大的文件里使用sed命令做大量的替换操作吗？</h4>

<p>常规做法：
<code>
sed s^old^new^g bigfile.txt
</code></p>

<p>现在你可以：
<code>
cat bigfile.txt | parallel --no-notice --pipe sed s^old^new^g
</code></p>

<h4>GREP 一个非常大的文本文件</h4>

<p>以前你可能会这样：</p>

<p><code>
grep pattern bigfile.txt
</code></p>

<p>现在你可以这样：
<code>
cat bigfile.txt | parallel --no-notice --pipe grep 'pattern'
</code></p>

<p>或者这样：
<code>
cat bigfile.txt | parallel --no-notice --block 10M --pipe grep 'pattern'
</code></p>

<p>这第二种用法使用了 –block 10M参数，这是说每个内核处理1千万行——你可以用这个参数来调整每个CUP内核处理多少行数据。</p>

<h4>压缩一个非常大的文件</h4>

<p>bzip2是比gzip更好的压缩工具，但它很慢！别折腾了，我们有办法解决这问题。</p>

<p>以前的做法：
<code>
cat bigfile.bin | bzip2 --best &gt; compressedfile.bz2
</code></p>

<p>现在这样：
<code>
cat bigfile.bin | parallel --no-notice --pipe --recend '' -k bzip2 --best &gt; compressedfile.bz2
</code></p>

<h2>扩展</h2>

<p>作为一个Python党，经常写一些<code>用过即弃</code>的边角料脚本</p>

<p>比如最近要把一个1T的文件汉字全部转换为拼音，初版当然是这样的:</p>

<h4>版本1</h4>

<p>```
with io.open(sys.argv[1], encoding=&lsquo;utf-8&rsquo;) as fp:</p>

<pre><code>for line in fp:
    print(lazy_pinyin(line))
</code></pre>

<p>```</p>

<p>lazy_pinyin的效率奇慢无比，这回陷入了一核有难，其它核+内存+磁盘全部看戏模式</p>

<p>作为一个初级合格的Python开发人员，你当然说要用process，于是我们有了第二版:</p>

<h4>版本2</h4>

<p>```
from multiprocessing import Pool
pool = Pool(16)
with io.open(sys.argv[1], encoding=&lsquo;utf-8&rsquo;) as fp:</p>

<pre><code>pool.map(lazy_pinyin, fp, 16)
pool.close()
pool.join()
</code></pre>

<p>```</p>

<p>嗯，很好，16个核都跑起来了；但是你有很快尴尬的发现，map把文件一把load进来，内存有难了</p>

<h4>~~~~</h4>

<p>作为一个初级合格的Python开发人员，你当然说不要一把读进来，要用chunk_read，一次读一块，或者更高级一点，直接用mmap映射进内存巴拉巴拉</p>

<h4>少年，这还是那个边角料脚本吗，你已经在它上面操心一个小时了，还能不能愉快的玩耍了</h4>

<p>让 parallel来拯救你</p>

<h4>版本3</h4>

<p>```
import fileinput</p>

<p>if <strong>name</strong> == &lsquo;<strong>main</strong>&rsquo;:</p>

<pre><code>for line in fileinput.input():
    lazy_pinyin(line)
</code></pre>

<p>```</p>

<p>然后执行:</p>

<p><code>
cat bigfile.txt| parallel --no-notice --pipe python pinyinconv.py &gt; pinyin.result
</code></p>

<p>享受所有CPU满负荷运载的工头压榨工人的快感吧</p>

<h2>一些扩展</h2>

<ul>
<li>为啥所有的parallel都带有一个奇怪的&mdash;no-notice?</li>
</ul>


<p>嗯，虽然这个作者非常非常好，但是他总是在命令前面输出一些慈善提示；当然我并不是讨厌这种做法，但看多了总有些疲劳，你懂的~~</p>

<ul>
<li>我有一些参数想传给程序，怎么办？</li>
</ul>


<p><code>
 seq 3|parallel --no-notice -q echo seq{}
</code></p>

<ul>
<li>这个命令很好，但是语法好像啰嗦了一些，还有其它的替代命令吗？</li>
</ul>


<p>嗯~ o(<em>￣▽￣</em>)o，还是有的，xargs有个-n参数，类似的效果，不过功能弱化很多，基本上是鸡肋</p>

<h2>参考:</h2>

<h4>手册:</h4>

<p><a href="https://www.gnu.org/software/parallel/parallel_tutorial.html">https://www.gnu.org/software/parallel/parallel_tutorial.html</a></p>

<h4>资料:</h4>

<p><a href="http://www.freeoa.net/osuport/sysadmin/use-gnu-parallel-multi-core-speed-up-cmd_2343.html">http://www.freeoa.net/osuport/sysadmin/use-gnu-parallel-multi-core-speed-up-cmd_2343.html</a></p>

<p>我的博客即将搬运同步至腾讯云+社区，邀请大家一同入驻：<a href="https://cloud.tencent.com/developer/support-plan?invite_code=1bnzu1pmog27t">https://cloud.tencent.com/developer/support-plan?invite_code=1bnzu1pmog27t</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Sort a Very Very Very Big File]]></title>
    <link href="http://happy123.me/blog/2018/05/03/how-to-sort-a-very-very-very-big-file/"/>
    <updated>2018-05-03T07:21:50+08:00</updated>
    <id>http://happy123.me/blog/2018/05/03/how-to-sort-a-very-very-very-big-file</id>
    <content type="html"><![CDATA[<p>sort -uo 一个1T的文件，让最高配的google cloud instance (48 core/512G)崩溃了<del>，可惜了我的$30，白白跑了那么长时间</del></p>

<p>网上搜索都是how to sort a big file，那我这个属于very very very big big big file了~~</p>

<p>不管是并行也好，管道也好，用了各种奇技淫巧就是敌不过人家 very very big~</p>

<p>不要跟我谈什么外排，归并，位图，bloom filter，redis hash去重，我就是不想折腾，最后只有分割手动外排搞定~~</p>

<h3>把大象装进冰箱分为几步？</h3>

<h3>三步:</h3>

<p>```
split -l 1000000000 huge-file small-chunk</p>

<p>for X in small-chunk*; do sort -u &lt; $X > sorted-$X; done</p>

<p>sort -u -m sorted-small-chunk<em> > sorted-huge-file &amp;&amp; rm -rf small-chunk</em> sorted-small-chunk*
```</p>

<h3>小TIPS:</h3>

<p>如果只要去重不要排序的话，尽量不要用 sort -u或者sort | uniq，这个是nLog(n)的效率，让人捉急。</p>

<p>可以利用awk的数组是内存hash表的特性，直接awk来做，前提是你内存够大，瞎估估需要十倍于数据的内存吧:</p>

<p><code>
cat xxxxx zzz | awk '{ if (!seen[$0]++) { print $0; } }' &gt; xxx_zzz.uniq.txt
</code></p>

<h3>PS:</h3>

<p>我后来又看了一下GNU Sort的实现描述，它说已经用了外排了，但是实际使用还是不给力，暂时迷惑中</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Get Intersection of Two Big Files]]></title>
    <link href="http://happy123.me/blog/2018/05/01/how-to-get-intersection-of-two-big-files/"/>
    <updated>2018-05-01T22:18:33+08:00</updated>
    <id>http://happy123.me/blog/2018/05/01/how-to-get-intersection-of-two-big-files</id>
    <content type="html"><![CDATA[<p>两个大文件，a.txt和b.txt两个文件的数据都是逐行呈现的， 如何求他们的交集、并集和差集。</p>

<p>用sort+uniq直接搞定:</p>

<h2>交集</h2>

<p><code>
$ sort a.txt | uniq &gt; aa.txt
$ sort b.txt | uniq &gt; bb.txt
$ cat aa.txt bb.txt | sort | uniq -d
</code></p>

<h2>并集</h2>

<p><code>
cat a.txt b.txt | sort | uniq
</code></p>

<h2>差集</h2>

<p><code>
$ sort a.txt | uniq &gt; aa.txt
$ sort b.txt | uniq &gt; bb.txt
$ cat aa.txt bb.txt bb.txt | sort | uniq -u
</code></p>

<ul>
<li>在开搞 bloom filter或者bitmap 或者grep -f之前可以先组合工具来一个</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Parallel All Cmds for Linux]]></title>
    <link href="http://happy123.me/blog/2018/05/01/how-to-parallel-all-cmds-for-linux/"/>
    <updated>2018-05-01T13:07:20+08:00</updated>
    <id>http://happy123.me/blog/2018/05/01/how-to-parallel-all-cmds-for-linux</id>
    <content type="html"><![CDATA[<p>grep 一个100GB的文件总是很有压力，怎么才能提速呢?</p>

<h3>瞎优化</h3>

<p><code>
LC_ALL=C fgrep -A 5 -B 5 'xxxxx.password' allpassseed.txt
</code></p>

<ul>
<li><p><code>LC_ALL=C</code>比<code>LC_ALL=UTF-8</code>要块</p></li>
<li><p>不需要正则的话，用fgrep可以提速</p></li>
</ul>


<h3>不过这样优化总是治标不治本，下面隆重推出linux 里面parallel all cmds的perl工具</h3>

<p><code>
cat allpassseed.txt |parallel  --pipe  --no-notice grep -f xxxxx.password
</code></p>

<p>使用parallel ，和不使用parallel直接grep。结果显而易见，相差 20 倍。这比用啥 ack，ag优化效果明显多了</p>

<h3>xargs也有一个-n的多核选项，可以作为备用</h3>

<p>```
$ time echo {1..5} |xargs -n 1  sleep</p>

<p>real    0m15.005s
user    0m0.000s
sys 0m0.000s
```</p>

<p>这一条xargs把每个echo的数作为参数传给sleep ，所以一共sleep了 1+2+3+4+5=15秒。</p>

<p>如果使用 -P 参数分给5个核，每个核各sleep 1,2,3,4,5秒，所以执行完之后总共sleep的5秒。</p>

<p>```
$ time echo {1..5} |xargs -n 1 -P 5 sleep</p>

<p>real    0m5.003s
user    0m0.000s
sys 0m0.000s
```</p>

<ul>
<li>引自:</li>
</ul>


<p><a href="https://www.jianshu.com/p/c5a2369fa613">https://www.jianshu.com/p/c5a2369fa613</a></p>
]]></content>
  </entry>
  
</feed>
