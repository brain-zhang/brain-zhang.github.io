<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ai | Living a Simple Life is a Happy Life]]></title>
  <link href="https://happy123.me/blog/categories/ai/atom.xml" rel="self"/>
  <link href="https://happy123.me/"/>
  <updated>2019-09-16T17:11:49+08:00</updated>
  <id>https://happy123.me/</id>
  <author>
    <name><![CDATA[brain-zhang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[吴恩达机器学习笔记-3]]></title>
    <link href="https://happy123.me/blog/2019/09/16/wu-en-da-ji-qi-xue-xi-bi-ji-3/"/>
    <updated>2019-09-16T16:55:56+08:00</updated>
    <id>https://happy123.me/blog/2019/09/16/wu-en-da-ji-qi-xue-xi-bi-ji-3</id>
    <content type="html"><![CDATA[<p>神经网络学习, 反向传播算法， 模型优化</p>

<!-- more -->

<h2 id="section">神经网络学习</h2>

<h4 id="section-1">为什么需要神经网络</h4>

<p>普通的逻辑回归模型，不能有效地处理大量的特征，这时候我们需要神经网络。</p>

<h4 id="section-2">神经元和大脑</h4>

<p>大脑是个通用处理机，同样的一部分大脑区域，可以处理声音、视觉、味觉等多种信号；</p>

<p>从某种意义上来说，如果我们能找出大脑的学习算法，然后在计算机上执 行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。人工智能的梦想就是：有一天能制造出真正的智能机器。</p>

<h4 id="section-3">模型表示</h4>

<p>&lt;img src=’https://i.loli.net/2018/12/01/5c01f2d2b8ac5.png’ width=500&gt;</p>

<p>第一层称为输入层（Input Layer），最后一 层称为输出层（Output Layer），中间一层称为隐藏层（Hidden Layers）。在神经网络中，参数又可被称为权重（weight）。我们为每一层都增加一个偏差单位（bias unit）</p>

<p>```
import numpy as np</p>

<p>def sigmoid(x):
    return 1/(1+np.exp(-x))</p>

<p>def net():
    # todo 确定输入和权重的维度
    X = np.array([[1],[-2],[3],[-4]])
    theta1 = np.random.uniform(size=(3, 4))
    hidden_input = sigmoid(np.matmul(theta1,X))
    print(‘hidden_input’,hidden_input)
    hidden_input = np.insert(hidden_input, 0, [1], axis=0)
    print(‘hidden_input’,hidden_input)
    theta2 = np.random.uniform(size=(1, 4))
    output = sigmoid(np.matmul( theta2,hidden_input))
    return output
```</p>

<h4 id="section-4">模型的表示实例</h4>

<p>从本质上讲，神经网络能够通过学习得出其自身的一系列特征。</p>

<p>在普通的逻辑回归中，我们被限制为使用数据中的原始特征 𝑥0,𝑥1,𝑥2,𝑥3 我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。</p>

<p>在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。</p>

<p>神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与（AND）、逻辑或（OR）。</p>

<p>```
# 实现AND函数
import numpy as np</p>

<p>def sigmoid(x):
    return 1/(1+np.exp(-x))</p>

<p>class Net():
    def <strong>init</strong>(self,theta):
        self.theta=theta
    def run(self,X):
        output = sigmoid(np.matmul(self.theta,X))
        return output</p>

<p>net = Net(np.array([[-30,20,20]]))
```</p>

<h4 id="section-5">多分类任务</h4>

<p>one-hot的基本思想：将离散型特征的每一种取值都看成一种状态，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。</p>

<p>如果分类问题有四个结果，
我们不会将y的取值为：0，1，2，3 而是会将y表示为一个1*4的向量</p>

<p><code>
import numpy as np
import tqdm
def onehot(x):
    unique_values = list(set(x))
    number_of_dimension = len(unique_values)
    onehot_features = np.zeros(shape=(len(x), number_of_dimension))
    for row in tqdm.tqdm(range(len(x))):
        onehot_features[row, unique_values.index(x[row])] = 1
    return onehot_features
</code></p>

<h2 id="section-6">反向传播算法</h2>

<h4 id="section-7">代价函数</h4>

<p>&lt;img src=’https://i.loli.net/2018/12/01/5c02139ee8b60.png’ width=500&gt;</p>

<p>通过代价函数来观察算法预测的结果与真实情况的误差有多大，与逻辑回归唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环
在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。</p>

<h4 id="section-8">反向传播</h4>

<p>计算神经网络预测结果的时候我们采用了一种正向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的 ℎ𝜃(𝑥)</p>

<p>计算代价函数的偏导数，我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。</p>

<p>老实说，反向传播的原理推导一直没搞明白，留一个通俗版先不求甚解：</p>

<p>https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/</p>

<p>最后，反向传播是为了提升神经网络学习模型中梯度下降的训练速度；是一种快速计算导数的方法；</p>

<h4 id="section-9">梯度校验</h4>

<p>名词跟梯度下降很相似，但是作用不一样；</p>

<p>当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。</p>

<p>为了避免这样的问题，我们采取一种叫做梯度的数值检验（Numerical Gradient Checking）的方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。</p>

<p>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 𝜃 ，我们计算出在  𝜃−𝜎 处和 𝜃+𝜎 的代价值（ 𝜎 是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 𝜃 处的代价值。</p>

<p>当 𝜃 是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对 𝜃1 进行检验的示例：</p>

<p><code>math
\frac{\partial}{\partial\theta_1} = \frac{J(\theta_1+\sigma_1,\theta_2,\theta_3,...,\theta_n)-J(\theta_1-\sigma_1,\theta_2,\theta_3,...,\theta_n)}{2\sigma}
</code></p>

<h4 id="section-10">随机初始化</h4>

<p>任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为 0，这样的 初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。</p>

<p>如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非 0 的数，结果也是一样的。</p>

<p>我们通常初始参数为正负 ε 之间的随机值，假设我们要随机初始一个尺寸为 10×11 的参数矩阵，代码如下：</p>

<p><code>
import numpy as np
a= np.random.rand(10,11) # 机初始一个尺寸为 10×11 的参数矩阵
print(a)
</code></p>

<h4 id="section-11">小结:</h4>

<ol>
  <li>参数的随机初始化</li>
  <li>利用正向传播方法计算所有的 hθ(x)</li>
  <li>编写计算代价函数 J 的代码</li>
  <li>利用反向传播方法计算所有偏导数</li>
  <li>利用数值检验方法检验这些偏导数</li>
  <li>使用优化算法来最小化代价函数</li>
</ol>

<h2 id="section-12">神经网络优化</h2>

<p>当我们建立一个神经网络学习模型之后，如何检验他到底好不好用？ 如果不好用，该怎样优化？</p>

<h4 id="section-13">假设检验</h4>

<p>为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。</p>

<ul>
  <li>线性回归模型，我们利用测试数据集计算代价函数J ；</li>
</ul>

<p><code>math
J_{test}(\theta)=\frac{1}{2m_{test}}\sum^{m_{test}}_{i=1}({h_\theta(x^{(i)}_{test})-y^{(i)}_{test}})^2
</code></p>

<ul>
  <li>逻辑回归模型，我们可以利用测试数据集来计算代价函数：</li>
</ul>

<p><code>math
J_{test}(\theta)=-\frac{1}{m_{test}}\sum^{m_{test}}_{i=1}(y^{(i)}_{test}\log{h_\theta(x^{(i)}_{test})}+(1-y^{(i)}_{test}）\log{(1-h_\theta(x^{(i)}_{test}))})
</code></p>

<h4 id="section-14">模型选择与训练集</h4>

<p>显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。</p>

<p>交叉验证集：训练集（train）用60%的数据，交叉验证集（validation）用20%的数据，测试集(test)用20%的数据</p>

<p>模型选择的方法为：</p>

<ol>
  <li>使用训练集训练出 10 个模型</li>
  <li>用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</li>
  <li>选取代价函数值最小的模型</li>
  <li>用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值）</li>
</ol>

<h4 id="section-15">诊断偏差和方差</h4>

<p>诊断偏差或是方差，即判断欠拟合还是过拟合;</p>

<ul>
  <li>训练集误差和交叉验证集误差都很高时：高偏差(欠拟合)</li>
  <li>训练集误差很小, 且交叉验证集误差远大于训练集误差时：高方差(过拟合)</li>
</ul>

<h4 id="section-16">正则化，偏差和方差</h4>

<p>选择 𝜆 的方法为：</p>

<ol>
  <li>使用训练集训练出 12 个不同程度正则化的模型</li>
  <li>用 12 模型分别对交叉验证集计算的出交叉验证误差</li>
  <li>选择得出交叉验证误差最小的模型</li>
  <li>运用步骤 3 中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与 𝜆 的值绘制在一张图表上：</li>
</ol>

<ul>
  <li>当 𝜆 较小时，训练集误差较小（过拟合）而交叉验证集误差较大</li>
  <li>随着 𝜆 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</li>
</ul>

<h4 id="section-17">学习曲线</h4>

<p>学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表。</p>

<p>思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。</p>

<p>当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。</p>

<h4 id="section-18">小结</h4>

<ul>
  <li>获得更多的训练实例——解决高方差</li>
  <li>尝试减少特征的数量——解决高方差</li>
  <li>尝试获得更多的特征——解决高偏差</li>
  <li>尝试增加多项式特征——解决高偏差</li>
  <li>尝试减少正则化程度 λ——解决高偏差</li>
  <li>
    <p>尝试增加正则化程度 λ——解决高方差</p>
  </li>
  <li>使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小</li>
  <li>使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[吴恩达机器学习笔记-2]]></title>
    <link href="https://happy123.me/blog/2019/09/11/wu-en-da-ji-qi-xue-xi-2/"/>
    <updated>2019-09-11T10:53:38+08:00</updated>
    <id>https://happy123.me/blog/2019/09/11/wu-en-da-ji-qi-xue-xi-2</id>
    <content type="html"><![CDATA[<p class="info">Logistic回归， 正则化</p>

<!-- more -->

<h2 id="logistic">1-Logistic回归</h2>

<h4 id="section">分类</h4>

<p>逻辑回归 (Logistic Regression)是分类问题的一个代表算法，这是目前最流行使用最广泛的一种学习算法。</p>

<p>我们将因变量(dependant variable)可能属于的两个类分别称为负向类（negative class）和 正向类（positive class），则因变量  𝑦∈0,1 ，其中 0 表示负向类，1 表示正向类。</p>

<p>分类问题下，可以采用逻辑回归的分类算法，这个算法的性质是：它的输出值永远在 0 到 1 之间。 它适用于标签 y 取值离散的情况，如：1 0 0 1。</p>

<h4 id="section-1">假设陈述</h4>

<p>分类问题，希望分类器的输出值在 0 和 1 之间，因此，假设函数需要满足预测值要在 0 和 1 之间。</p>

<p>回归模型的假设是：</p>

<script type="math/tex; mode=display">
h_\theta(x)=g(\theta^TX)
</script>

<p>其中：</p>

<ul>
  <li>
    <p>X 代表特征向量</p>
  </li>
  <li>
    <p>g 代表逻辑函数（logistic function）, 是一个常用的逻辑函数为 S形函数（Sigmoid function），公式为：</p>
  </li>
</ul>

<script type="math/tex; mode=display">
g(z) = \frac{1}{1+e^{-z}}
</script>

<ul>
  <li>python 代码实现sigmoid函数：</li>
</ul>

<p><code>
import numpy as np
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
</code></p>

<p>结合起来，获得逻辑回归的假设：</p>

<script type="math/tex; mode=display">
h_\theta(x) =  \frac{1}{1+e^{-\theta^TX}}
</script>

<table>
  <tbody>
    <tr>
      <td>𝜃(𝑥) 的作用是，对于给定的输入变量，根据选择的参数计算输出变量为1 的可能性 （estimated probablity），即  ℎ𝜃(𝑥)=𝑃(𝑦=1</td>
      <td>𝑥;𝜃) 。</td>
    </tr>
  </tbody>
</table>

<h4 id="section-2">代价函数</h4>

<p>逻辑回归的代价函数为：</p>

<script type="math/tex; mode=display">
J(\theta)= \frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}), y^{(i)})
</script>

<p>其中:</p>

<script type="math/tex; mode=display">
Cost(h_\theta(x), y)=-y\times{log(h_\theta(x))}-(1-y)\times{log(1-h_\theta(x))}
</script>

<p>代入代价函数:</p>

<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{m}\sum^m_{i=1}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]
</script>

<ul>
  <li>逻辑回归代价函数的Python代码实现：</li>
</ul>

<p><code>
import numpy as np
def cost(theta, X, y):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    return np.sum(first - second) / (len(X))
</code></p>

<h4 id="section-3">简化代价函数和梯度下降</h4>

<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
</script>

<p>这个更新规则和之前用来做线性回归梯度下降的式子是一样的， 但是假设的定义发生了变化。即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。</p>

<h4 id="section-4">多分类任务 一对多</h4>

<p>邮件归类， 假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，就有了一个四分类问题：其类别有四个，分别用 y=1、y=2、y=3、y=4 来代表。</p>

<p>多分类的关键就是构建多个逻辑分类函数；具体：</p>

<p>我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作 ℎ(1)𝜃(𝑥)。接着，类似地第我们选择另一个类标记为 正向类（y=2），再将其它类都标记为负向类，将这个模型记作  ℎ(2)𝜃(𝑥) ,依此类推。 最后我们得到一系列的模型简记为：</p>

<script type="math/tex; mode=display">
h^{(i)_\theta(x)} = p(y=i|x;\theta)
</script>

<p>最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器： ℎ(𝑖)𝜃(𝑥) ， 其中 i对应每一个可能的y=i，最后，为了做出预测，我们给出输入一个新的 x 值做预测。我们要做的就是在我们三个分类器里面输入 x，然后我们选择一个让  ℎ(𝑖)𝜃(𝑥) 最大的 i，即</p>

<script type="math/tex; mode=display">
\max_ih^{(i)_\theta(x)}
</script>

<h2 id="section-5">2-正则化</h2>

<h4 id="section-6">过拟合问题</h4>

<p>就以多项式理解，x 的次数越高，拟合的越好，但相应的预测的能力就可能变差。</p>

<p>如何解决？</p>

<ul>
  <li>
    <p>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如 PCA, LDA），缺点是丢弃特征的同时，也丢弃了这些相应的信息；</p>
  </li>
  <li>
    <p>正则化。 保留所有的特征，但是减少参数的大小（magnitude），当我们有大量的特征，每个特征都对目标值有一点贡献的时候，比较有效。</p>
  </li>
  <li>
    <p>还有一个解决方式就是增加数据集,因为过拟合导致的原因就过度拟合测试数据集, 那么增加数据集就很大程度提高了泛化性了.</p>
  </li>
</ul>

<h4 id="section-7">代价函数</h4>

<p>正则化的基本方法：对高次项添加惩罚值，让高次项的系数接近于0。</p>

<p>假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：</p>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m} [ \sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2} + \lambda\sum_{j=1}^n\theta^2_j  ]
</script>

<p>其中 𝜆 又称为正则化参数（Regularization Parameter）</p>

<p>```
import numpy as np
def mseWithRegular(predict, y, w, lmd=0.1):
    ‘’’
        predict: 模型输出
        y: 真实标签
        w: 模型权重
        lmd: 正则化参数
    ‘’’
    constrct_loss = np.sum((predict - y) ** 2)
    experience_loss = lmd * np.sum(w ** 2)
    loss = (constrct_loss + experience_loss) / (2 * len(predict))
    return loss</p>

<p>predict = np.array([1, 1.5, 2])
y = np.array([0.9, 1.4, 2.1])
w = np.array([[1], [1], [1]])
mseWithRegular(predict, y, w)
```</p>

<p>如果选择的正则化参数 𝜆 过大，则会把所有的参数都最小化了，导致模型变成  ℎ𝜃(𝑥)=𝜃0 ，造成欠拟合。</p>

<p>所以对于正则化，我们要取一个合理的λ的值，这样才能更好的应用正则化。</p>

<h4 id="section-8">线性回归正则化</h4>
<p>对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程</p>

<p>正则化线性回归的代价函数为：</p>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m} [ \sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2} + \lambda\sum_{j=1}^n\theta^2_j ]
</script>

<ul>
  <li>梯度下降使代价函数最小化</li>
</ul>

<script type="math/tex; mode=display">
\theta_j := \theta_j (1-a\frac{\lambda}{m})- \alpha \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
</script>

<ul>
  <li>正规方程来求解正则化线性回归模型</li>
</ul>

<p>TODO: 暂时没有理解</p>

<h4 id="section-9">逻辑回归正则化</h4>

<p>针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：梯度下降法，更高级的优化算法需要你自己设计代价函数 𝐽(𝜃) 。</p>

<p>给代价函数增加一个正则化的表达式，得到代价函数:</p>

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{m}\sum^m_{i=1}[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)}log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta^2_j
</script>

<p>代码实现:</p>

<p>```
import numpy as np
def sigmoid(x, derivative=False):
    sigm = 1. / (1. + np.exp(-x))
    if derivative:
        return sigm * (1. - sigm)
    return sigm</p>

<p>def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))
    return np.sum(first - second) / (len(X)) + reg
```</p>

<p>最后，它的梯度下降看上去同正则化的线性回归一样，但是由于假设ℎ𝜃(𝑥)=𝑔(𝜃𝑇𝑋) ，所以与线性回归不同。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[吴恩达机器学习笔记-1]]></title>
    <link href="https://happy123.me/blog/2019/09/01/wu-en-da-ji-qi-xue-xi-bi-ji/"/>
    <updated>2019-09-01T15:59:41+08:00</updated>
    <id>https://happy123.me/blog/2019/09/01/wu-en-da-ji-qi-xue-xi-bi-ji</id>
    <content type="html"><![CDATA[<p class="info">这个系列教程大名鼎鼎，之前我都是用到啥就瞎试一通；最近花了两个周，认认真真把这些基础知识重新学了一遍；做个笔记；
苏老泉二十七始发愤，我这比他还落后；不过求知的旅途，上路永远不嫌晚，我一直在路上；</p>

<!-- more -->

<h2 id="supervised-learning">1-监督学习（Supervised Learning)</h2>

<p>根据训练数据是否拥有标记信息，学习任务可大致被分为两类：</p>

<ul>
  <li>
    <p>监督学习（Supervised Learning）监督学习的代表是回归和分类。</p>

    <ul>
      <li>回归:预测连续值的模型: 已知房子大小和房价数据集，预测某一房子的价格</li>
      <li>分类:预测离散值的模型: 已知肿瘤性质和大小数据集，预测肿瘤是否良性</li>
    </ul>
  </li>
  <li>
    <p>无监督学习（Unsupervised Learning） 无监督学习的代表是聚类。</p>
  </li>
</ul>

<h2 id="section">2-单变量线性回归</h2>

<h4 id="section-1">模型表示</h4>

<script type="math/tex; mode=display">
h_{\theta}(x) = \theta_{0} + \theta_{1}x
</script>

<h4 id="section-2">代价函数</h4>

<p>求两个值，使模型最为匹配当前数据集；求解匹配度的过程提炼出代价函数；代价函数值越小，匹配度越高</p>

<script type="math/tex; mode=display">
J(\theta_{0}, \theta_{1}) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2}
</script>

<p>当𝜃1=0时，代价函数为一抛物线；
当𝜃0，𝜃1都不为0时，代价函数为一三维曲面；</p>

<h4 id="section-3">自动求解代价函数</h4>

<p>我们我们有函数  𝐽(𝜃0,𝜃1) , 可以不断的调整  𝜃0  和  𝜃1 , 来使得  𝐽(𝜃0,𝜃1)  , 直到  𝐽(𝜃0,𝜃1)  达到最小值为止</p>

<p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数 𝐽(𝜃0,𝜃1) 的最小值。</p>

<p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合 (𝜃0,𝜃1,……,𝜃𝑛)  ，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到抵达一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。</p>

<p>帅气的梯度下降算法公式:</p>

<script type="math/tex; mode=display">
\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial{\theta_{j}}}J(\theta)
</script>

<p>对 𝜃 赋值，使得  𝐽(𝜃) 按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中 𝛼 是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。</p>

<ul>
  <li>如果 𝛼 太小了，即我的学习速率太小，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。</li>
  <li>如果 𝛼 太大，那么梯度下降法可能会越过最低点，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，最终会导致无法收敛，甚至发散。</li>
</ul>

<h2 id="section-4">3-矩阵和向量</h2>

<h4 id="x2">一个2X2矩阵</h4>

<p>```
import numpy as np
a=np.array([[1, 2], [3, 4]])</p>

<p>```</p>

<h4 id="section-5">向量是列数为1的特殊矩阵:</h4>

<p>```
b = np.array(np.zeros((3,1)))</p>

<p>```</p>

<h4 id="section-6">矩阵的加法</h4>

<p>行列数相等的才可以做加法，两个矩阵相加就是行列对应的元素相加。</p>

<p>```
import numpy as np
a = np.mat([[1,0],[2,5],[3,1]])
b = np.mat([[4,0.5],[2,5],[0,1]])
print (“a: \n”,a, “\nb: \n”,b)
print (“a+b: \n”,a+b)  # a + b，矩阵相加</p>

<p>```</p>

<h4 id="section-7">矩阵的标量乘法</h4>

<p>矩阵和标量的乘法也很简单,就是矩阵的每个元素都与标量相乘。</p>

<p>```
print (“a: \n”,a)
print (“3<em>a: \n”,3</em> a)  #矩阵标量乘法</p>

<p>```</p>

<h4 id="section-8">向量乘法</h4>
<p>m×n 的矩阵乘以 n×1 的向量，得到的是 m×1 的向量</p>

<p>```
import numpy as np
a = np.mat([[-1,2],[2,3]])
c = np.mat([[3],[4]])
ac = a * c</p>

<p>```</p>

<h4 id="section-9">矩阵乘法的性质</h4>
<ul>
  <li>矩阵的乘法不满足交换律： 𝐴×𝐵≠𝐵×𝐴</li>
  <li>矩阵的乘法满足结合律。即： 𝐴×（𝐵×𝐶）=（𝐴×𝐵）×𝐶</li>
  <li>在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的 1,我们称这种矩阵为单位矩阵．它是个方阵，一般用 I 或者 E 表示，本讲义都用 I 代表单位矩阵，从左上角到右下角的对角线（称为主对角线）上的元素均为 1 以外全都为 0。</li>
</ul>

<h4 id="section-10">逆矩阵</h4>

<p>矩阵 A 是一个 m×m 矩阵（方阵），如果有逆矩阵，则：𝐴𝐴−1=𝐴−1𝐴=𝐼</p>

<p>没有逆矩阵的矩阵, 称为奇异 (singlar/degenerate)矩阵</p>

<p>```
import numpy as np</p>

<p>a = np.mat([[1,2],[3,4]])
print (‘a:\n’,a)
res = np.linalg.inv(a)
print(‘a inverse:\n’, res)</p>

<p>```</p>

<p>备注: 再octave中，可以用pinv函数(伪逆矩阵)对奇异矩阵求逆；</p>

<h4 id="section-11">矩阵转置</h4>

<p>设 A 为 m×n 阶矩阵（即 m 行 n 列），第 i 行 j 列的元素是 a(i,j)，即：A=a(i,j) 定义 A 的转置为这样一个 n×m 阶矩阵 B，满足 B=a(j,i)，即 b (i,j)=a (j,i)（B 的第 i 行第 j 列元素是 A 的第 j 行第 i 列元素），记  𝐴𝑇=𝐵 。</p>

<p>```
a = np.mat([[1,2],[3,4]])
print (‘a:\n’,a)
res = a.T
print(‘a transpose:\n’, res)</p>

<p>```</p>

<h2 id="section-12">4-多变量线性回归</h2>

<ul>
  <li>引入多种特征后的假设h模型</li>
</ul>

<script type="math/tex; mode=display">
h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{n}x_{n}
</script>

<p>此时模型中的参数是一个 n+1 维的向量，任何一个训练实例也都是 n+1 维的向量，特征矩阵 X 的维度是 m*(n+1)。 因此公式可以简化为：</p>

<script type="math/tex; mode=display">
h_{\theta}(x) = \theta^TX
</script>

<h4 id="section-13">多变量梯度下降</h4>

<p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价 函数是所有建模误差的平方和，即：</p>

<script type="math/tex; mode=display">
J(\theta_{0}, \theta_{1}...\theta_{n}) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2}
</script>

<p>```
# 代价函数的python代码实现
def Cost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))</p>

<p>```</p>

<h4 id="section-14">梯度下降 - 特征缩放</h4>

<p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这可以帮助梯度下降算法更快地收敛。</p>

<p>解决的方法是尝试将所有特征的尺度都尽量缩放到-1 到 1 之间。</p>

<p>最简单的方法是令：</p>

<script type="math/tex; mode=display">
x_n = \frac{x_n - \mu_n}{s_n}
</script>

<p>其中,  𝜇𝑛 是平均值， 𝑠𝑛 是标准差。</p>

<h4 id="section-15">梯度下降 - 学习率</h4>

<ul>
  <li>如果学习率 𝛼 过小，则达到收敛所需的迭代次数会非常高；</li>
  <li>如果学习率 𝛼 过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</li>
</ul>

<p>通常可以考虑尝试些学习率： 0.01，0.03，0.1，0.3，1，3，10; 3倍增长</p>

<h4 id="section-16">特征与多项式回归</h4>

<p>如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。因为幂运算很容易拉大特征之间尺度的差距</p>

<h4 id="section-17">正规方程</h4>

<p>假设我们的训练集特征矩阵为 X（包含了 𝑥0=1 ）并且我们的训练集结果为向量 y， 则利用正规方程解出向量</p>

<script type="math/tex; mode=display">
\theta = (X^TX)^{-1}X^Ty
</script>

<p>只要特征变量的数目并不大，标准方程是一个很好的计算参数 𝜃 的替代方法。具体地说，只要特征变量的数量小于一万，通常使用标准方程法，而不使用梯度下降法。</p>
]]></content>
  </entry>
  
</feed>
